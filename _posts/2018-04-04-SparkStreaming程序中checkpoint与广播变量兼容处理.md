---
layout: post
title: "SparkStreaming程序中checkpoint与广播变量兼容处理"
date: 2018-04-04
description: 本文主要关于Spark Streaming程序中同时使用checkpoint与广播变量的方法，此文中代码在上文“SparkStreaming输出数据到Kafka--Kafka连接池的使用”的基础上进行优化，使其能够同时使用checkpoint记录读取上游kafka的offset以及kafkaPool广播变量。
categories:
- BigData
tags:
- Spark
- Kafka
---
> 上文中说明了如何使用kafka连接池来优化程序，但是在上文中预留了一个问题，就是当使用上文的方式下发KafkaPool广播变量时，如果Spark Streaming程序中也使用了checkpoint，则如果程序中断而重启程序，广播变量无法从checkpoint中恢复，会出现“java.lang.ClassCastException:B cannot be cast to KafkaPool”问题，所以在此篇文章中，对此问题进行解决。  
> **注意: 本文中使用的版本是spark2.2.1和kafka0.10.1.1**  
  
# 背景  
在spark信令处理程序中使用checkpoint主要是因为从源头读取kafka数据的时候记录offset，防止数据丢失；并且目前是做的是容器化的集群，如果集群down了，会自动重启容器并且也能把程序恢复。  
不过对于Spark Streaming中防止数据丢失可以有两种方式:
1. 使用Spark Streaming的checkpoint机制  
2. 自己维护kafka offset  
  
但是，有人说checkpoint有弊端，并且我也遇到了序列化的这个问题。在查资料的过程中，看到有评论说checkpoint与广播变量就是不能同时使用（这是不对的），所以也思考过要不要改成自己手动维护offset，而且发现有好多人也这么做了。不过我们代码更新迭代并不频繁，不会被checkpoint影响太大，所以还是决定再试试使用checkpoint，终于好不容易也让我找到了解决方法，真是巨开心,下面就说下怎么解决的。  
  
# 解决-示例说明  
## 参考例子  
在Spark Streaming中，目前为止累加器和广播变量确实是无法从checkpoint恢复的。但是如果在程序中既使用到checkpoint又使用了累加器和广播变量的话，最好对累加器和广播变量做懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。  
解决方法其实就在spark官方的项目的examples中，访问请戳: [RecoverableNetworkWordCount](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala) ，它是广播变量和累加器与checkpoint兼容的一个例子。下面我就把代码摘出来记录一下。  
### 第一步：用单例模式来获取或生成广播变量和累加器  
```scala  
/**
 * Use this singleton to get or register a Broadcast variable.
 * 单例模式获取广播变量wordBlacklist
 */
object WordBlacklist {
  @volatile private var instance: Broadcast[Seq[String]] = null
  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          val wordBlacklist = Seq("a", "b", "c")
          instance = sc.broadcast(wordBlacklist)
        }
      }
    }
    instance
  }
}
/**
 * Use this singleton to get or register an Accumulator.
 * 累加器
 */
object DroppedWordsCounter {
  @volatile private var instance: LongAccumulator = null
  def getInstance(sc: SparkContext): LongAccumulator = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          instance = sc.longAccumulator("WordsInBlacklistCounter")
        }
      }
    }
    instance
  }
}
```
### 第二步：在Spark主程序中使用  
在主程序的driver端位置使用，懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。
```scala  
object RecoverableNetworkWordCount {
  // 这个是用来生成StreamingContext对象的用户自定义的方法
  def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
    : StreamingContext = {
    // If you do not see this printed, that means the StreamingContext has been loaded from the new checkpoint
    // 如果没有打印出这句话，说明是使用检查点元数据恢复一个StreamingContext
    println("Creating new context")
    val outputFile = new File(outputPath)
    if (outputFile.exists()) outputFile.delete()
    val sparkConf = new SparkConf().setAppName("RecoverableNetworkWordCount")
    // 创建sparkContext，1秒一个批次
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    // 设置checkpoint
    ssc.checkpoint(checkpointDirectory)
    // Create a socket stream on target ip:port and count the words in input stream of \n delimited text (eg. generated by 'nc')
    // 这里是测试的socket stream
    val lines = ssc.socketTextStream(ip, port)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
    wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =>
      // Get or register the blacklist Broadcast 广播变量
      val blacklist = WordBlacklist.getInstance(rdd.sparkContext)
      // Get or register the droppedWordsCounter Accumulator 累加器
      val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)
      // Use blacklist to drop words and use droppedWordsCounter to count them 使用广播变量和累加器
      val counts = rdd.filter { case (word, count) =>
        if (blacklist.value.contains(word)) {
          droppedWordsCounter.add(count)
          false
        } else {
          true
        }
      }.collect().mkString("[", ", ", "]")
      val output = s"Counts at time $time $counts"
      println(output)
      println(s"Dropped ${droppedWordsCounter.value} word(s) totally")
      println(s"Appending to ${outputFile.getAbsolutePath}")
      Files.append(output + "\n", outputFile, Charset.defaultCharset())
    }
    ssc
  }
  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println(s"Your arguments were ${args.mkString("[", ", ", "]")}")
      System.err.println(
        """
          |Usage: RecoverableNetworkWordCount <hostname> <port> <checkpoint-directory>
          |     <output-file>. <hostname> and <port> describe the TCP server that Spark
          |     Streaming would connect to receive data. <checkpoint-directory> directory to
          |     HDFS-compatible file system which checkpoint data <output-file> file to which the
          |     word counts will be appended
          |In local mode, <master> should be 'local[n]' with n > 1
          |Both <checkpoint-directory> and <output-file> must be absolute paths
        """.stripMargin
      )
      System.exit(1)
    }
    val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args
    // 使用StreamingContext.getOrCreate来创建StreamingContext对象，传入的第一个参数是checkpoint的存放目录，第二参数是生成StreamingContext对象的用户自定义方法。
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () => createContext(ip, port, outputPath, checkpointDirectory))
    ssc.start()
    ssc.awaitTermination()
  }
}
```  
# 解决-SparkStreaming+kafkaPool程序修改(兼容checkpoint和广播变量kafkaPool)  
上述内容即如何将checkpoint与广播变量或累加器兼容的例子，下面则结合上述例子，对上文([SparkStreaming写数据到Kafka](https://leafming.github.io/bigdata/2018/04/02/SparkStreaming写数据到Kafka-Kafka连接池的使用/))的程序做修改来解决“java.lang.ClassCastException:B cannot be cast to KafkaPool”的问题。  
## 第一步：包装KafkaProducer-创建Kafka连接池（不变）
对于上文([SparkStreaming写数据到Kafka](https://leafming.github.io/bigdata/2018/04/02/SparkStreaming写数据到Kafka-Kafka连接池的使用/))中的程序，第一步保持不变，创建class KafkaPool 以及object KafkaPool，将KafkaProducer以lazy val的方式进行包装。  
```scala
import java.util.concurrent.Future
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, RecordMetadata}
//scala中，类名之后的括号中是构造函数的参数列表，() =>是传值传参，KafkaProducer
class KafkaPool[K, V]( createProducer: () => KafkaProducer[K,V])  extends Serializable{
  //使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法
  //后续在spark主程序中使用时，将kafkapool广播出去到每个executor里面了，然后到每个executor中，当用到的时候，会实例化一个producer，这样就不会有NotSerializableExceptions的问题了。
  lazy val producer = createProducer()
  def send(topic: String, key: K, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, key, value))
  def send(topic: String, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, value))
}
object KafkaPool{
  import scala.collection.JavaConversions._
  def apply[K, V](config: Map[String, Object]): KafkaPool[K, V] = {
      val createProducerFunc = () => {
        System.setProperty("java.security.auth.login.config","kafka_client_jaas.conf")
        val producer = new KafkaProducer[K, V](config)
        sys.addShutdownHook {
          //当发送ececutor中的jvm shutdown时，kafka能够将缓冲区的消息发送出去。
          producer.close()
        }
        producer
      }
      new KafkaPool(createProducerFunc)
  }
  def apply[K, V](config: java.util.Properties): KafkaPool[K, V] = apply(config.toMap)
}
```
## 第二步：对广播变量懒实例化操作，使用单例模式来获取广播变量KafkaPool
这里较上文([SparkStreaming写数据到Kafka](https://leafming.github.io/bigdata/2018/04/02/SparkStreaming写数据到Kafka-Kafka连接池的使用/))第一种方式中的直接下发广播变量有所区别，而是创建来一个GetKafkaPoolBroadcast的getKafkaPool方法，用于在主程序中driver端调用此方法时再获取或生成广播变量。
```scala  
import java.util.Properties
import org.apache.spark.SparkContext
import org.apache.spark.internal.Logging
import org.apache.spark.broadcast.Broadcast

object GetKafkaPoolBroadcast extends Logging {
  @volatile private var kafkapool: Broadcast[KafkaPool[String, String]]  = null
  def getKafkaPool(sc: SparkContext,proKafkaBrokerAddr:String): Broadcast[KafkaPool[String, String]] = {
    if (kafkapool == null) {
      synchronized {
        if (kafkapool == null) {
          val kafkaProducerConfig = {
            val props = new Properties()
            props.put("metadata.broker.list",proKafkaBrokerAddr)
            props.put("security.protocol","SASL_PLAINTEXT")
            props.put("sasl.mechanism","PLAIN")
            props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
            props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")
            props.put("bootstrap.servers",proKafkaBrokerAddr)
            props
          }
          log.warn("kafka producer init done!")
          kafkapool = sc.broadcast(KafkaPool[String, String](kafkaProducerConfig))
        }
      }
    }
    kafkapool
  }
}
```
## 第三步：在主程序中使用GetKafkaPoolBroadcast的getKafkaPool获取或生成广播变量  
在主程序中，driver端的位置调用此方法，这样可以使广播变量在driver失败重启时能够重新示例化。
```scala  
    //保存处理后的数据到kafka
    writeDStrem.foreachRDD(rdd => {
      // driver端运行，涉及操作：广播变量的初始化和更新
      if (rdd.isEmpty) {
        logInfo(" No Data in this batchInterval --------")
      } else {
        val start_time = System.currentTimeMillis()
        // Get or register the kafkaPool Broadcast 获取或生成广播变量kafkaPool
        val kafkaProducer: Broadcast[KafkaPool[String, String]]=GetKafkaPoolBroadcast.getKafkaPool(rdd.sparkContext,proKafkaBrokerAddr)
        rdd.foreach(record=>{
          kafkaProducer.value.send(proKafkaTopicName,record)
        })
        competeTime(start_time, "Processed data write to KAFKA")
      }
    })
```
这样，就不会再尝试从checkpoint中恢复广播变量，而可以避免“java.lang.ClassCastException:B cannot be cast to KafkaPool”这个问题啦。  
至此，本篇内容完成。  
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～  
©转载请注明
