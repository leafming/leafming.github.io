<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title>机器学习入门自学-描述统计学入门</title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/05/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%87%AA%E5%AD%A6-%E6%8F%8F%E8%BF%B0%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%85%A5%E9%97%A8/</url>
      <content type="text">
  本文关于自学udacity机器学习课程的自学笔记，为第一部分，描述统计学入门。


2018-05-08-研究方法入门
基本概念
抽象概念（constructs）  
操作性定义：一种将构造（constructs）转变为我们可衡量的变量的方式；一种用我们衡量它的方式描述变量的方式。

总体参数μ：描述整个总体的平均值
样本统计量：样本的平均值
样本大小n
抽样误差μ-：样本平均值和总体平均值之间的差异。在使用一个样本对一个总体进行推理时，样本的平均值可能不会完全等于总体的平均值。 
用来估计μ，估计值是对μ的最佳猜测。

好样本：大、随机无偏差
随机样本：每个对象被选中的概率都是一样的。
可视化关系-eg 散点图
散点图：
1.x轴上变量：自变量（independent variable）或预测变量（predictor variable）
2.y轴上变量：因变量（dependent variable）即结果（outcome）

基本概念-变量
在试验中，研究人员操纵 (自) 变量，测量 (因) 变量的变化，然后尝试控制(潜伏)变量。
简单点说，自变量是“原因”，而因变量就是“结果”。在实验中，自变量是由实验者操纵、掌握的变量。因变量是因为自变量的变化而产生的现象变化或结果。因此自变量和因变量的相互依存的，没有自变量就无所谓因变量，没有因变量也无所谓自变量。
1.自变量:
自变量（Independent variable）一词来自数学。在数学中，y=f(x)。在这一方程中自变量是x，因变量是y。将这个方程运用到心理学的研究中，自变量是指研究者主动操纵，而引起因变量发生变化的因素或条件，因此自变量被看作是因变量的原因。自变量有连续变量和类别变量之分。如果实验者操纵的自变量是连续变量，则实验是函数型实验。如实验者操纵的自变量是类别变量，则实验是因素型的。在心理学实验中，一个明显的问题是要有一个有机体作为被试对刺激作反应。显然，这里刺激变量就是自变量。
2.因变量:
因变量（dependent variable）函数中的专业名词，也叫函数值。函数关系式中，某些特定的数会随另一个（或另几个）会变动的数的变动而变动，就称为因变量。如：Y=f(X)。此式表示为：Y随X的变化而变化。Y是因变量，X是自变量。另外“因变量”也特指心理实验中的专业名词。
3.潜在变量（外部因素）:
a.可为变量之间观察到的关系提供可能的另一种解释
b.是会影响我们衡量的两个或多个变量之间关系的因素
c.在我们做出确定的因果声明之前，必须在试验中加以控制
d.使从观察性研究的数据中确定因果关系变得困难

相关并不代表因果
correlation does not imply causation. - 相关并不代表因果  
show releation（关系） =&amp;gt; observational studies surveys（观察性研究）  
show causation（因果关系 即特定因素） =&amp;gt; controued experiment（对照实验，实验性研究）

实验性研究和观察性研究
实验性研究和观察性研究的区别:
观察性研究：是非随机化的研究,在自然状态下对研究对象的特征进行观查、记录，并对结果进行描述和对比分析的研究。
实验性研究：就是人为地进行干预措施，而收集到结果的分析性研究。
实验性研究是人为研究，观察性研究是自然研究。
拓展：
（1）观察性研究中的基本要素：一个是研究对象，另一个是研究因素。在描述性研究中，研究因素是影响因素；在分析性研究中，研究因素称为危险因素或暴露因素。
（2）观察性研究的特征概括：在研究中，不向研究对象施加任何实验因素（干预因素），可以将观察对象按某种特征分组，但不需随机分组。
（3）实验性研究是在控制的条件下系统的操纵某种变量的变化，来研究这种变量的变化对其他变量所产生的影响。

调查方式：调查问卷
1.不理解内容 =&amp;gt; 应答偏差
2.拒绝回答 =&amp;gt; 无应答偏差

控制因素：盲法
安慰剂（无效药）
1.单盲：参与者不知道自己是无效药
2.双盲：都不知道

小习题
假设我们将患有失眠症的人随机分配到两个治疗方法中。在一个方法中，被试获得 20 毫克唑吡旦（安比恩）。在另一种方法中，被试获得了一个安慰剂药片。被试不知道他们服用的是哪种类型的药片。服药后，被试在睡眠实验室入睡以帮助控制潜在变量。第二天早上，在与心理学家的对话中，他们对自己的睡眠质量进行了从 1 到 10 的评分（1 表示“非常差”，10 表示“非常好”）。心理学家同样不知道他们各自服用了哪种药物。唑吡旦组的被试报告的睡眠质量（均值 = 8）优于安慰剂组（均值 = 5）。
关于这个情景以下哪个说法是正确的？选择所有适用项。
A这是一项观察性研究示例，因此我们无法对唑吡旦的有效性得出因果结论。
B参与者可以看出他们服用的是什么药片。
C药片类型（唑吡旦或安慰剂）为因变量。
D此研究为实验性研究，我们可以对唑吡旦的有效性得出因果结论。
E该实验使用了双盲对照组，因为参与者和心理学家都不知道每个人服用的药片类型。
F唑吡旦组与安慰剂组报告的睡眠质量之间的差异，可能不是由唑吡旦引起的。
G睡眠质量的操作定义使用一个满分为100分的度量表。
H药丸类型（唑吡旦或安慰剂）是自变量。
I治疗成果的操作定义使用一个满分为10分的度量表。
J心理学家不知道被试服用的是那种药丸。
K这项研究表明，在其他条件相同的情况下，唑吡旦比安慰剂更能改善睡眠质量。

答案:DEHJK

习题解释:
此研究为实验性研究，我们可以对唑吡旦的有效性得出因果结论;该实验使用了双盲对照组，因为参与者和心理学家都不知道每个人服用的药片类型。
The independent variable is whatever differed between the experiment group and control group. What was the independent variable in this experiment?
药丸类型（唑吡旦或安慰剂）是自变量。
In an experimental study, unlike an observational study, it is valid to make causal conclusions since randomization minimizes the effect of lurking variables.What conclusions can you draw from the results of this experimental study?
这项研究表明，在其他条件相同的情况下，唑吡旦比安慰剂更能改善睡眠质量。
Note that while the operational definition of quality of sleep is the 10-point scale, the way we actually measure success comes from the difference in quality between groups.
治疗成果的操作定义不是使用一个满分为10分的度量表。而是参与者的睡眠质量。

2018-05-11-数据可视化
Σ号表示总和，它是希腊字母大写西格玛。
f代表频率（计数），p代表比例。
在此情况下，n（样本中的数字）等于此次研究中的总人数。
频率表
frequency 在中文中指频数，而relative frequency指的是频率（相对频数）
频数（频率）：个数
相对频数（相对频率）：比例或百分比,频数除以样本数
1.展示方式1:用比例(小数)表示：0&amp;lt;=比例&amp;lt;=1 ,如0.23
2.展示方式2:百分比表示：0%&amp;lt;=百分比&amp;lt;=100% ,如23%

对于任何频率表，所有相对频率之和应当等于1。
表格的行数,取决于你如何去整理数据。
区间（interval）、容器（bin）、桶（bucket）:每行选择一个范围，如0-19、20-39
组距：是指对频率进行计数的区间，如为20 
一组混乱无章的数据，可以通过频率表描述数据可视化。
直方图
X轴上：变量，是可以量化的值，可划分组距（如年龄、价格（元）、长度（米）的值）-区分柱状图 
Y轴上：只是频率
Frequency is on the y-axis; countries and colors are categorical data. With histograms, the x-axis should be numerical.
直方图两个轴的交叉点是原点，笛卡尔坐标是(0,0)。 
 
组距大小（区间大小），可以创建任何组距大小的直方图，Interactivate直方图软件。
组距越大，每个区间内观察值越多。
组距大小取决于需要多大的分组、回答什么养的问题、需要多少详细信息。选择的组距大小在某些情况下可能会牺牲一些信息。
每个长条上的数字可代表其频率。 
通常，当我们使用直方图做数据可视化时，如果我们增大组距，频率会怎样？–变大：As we make the bin size bigger, more values will fall inside that bin.

直方图和柱状图区别

柱状图（BAR GRAPH）：
柱之间的空间表示每个柱都是独特的类别，无法更改组距；柱状图可能会根据需要回答的问题，按照一定顺序排列柱状物，这样可以轻松的从x轴寻找到任何内容；柱状图的形状是任意的，取决于如何在x轴上排列各个类别；x轴上的变量通常是分类或者定性的(categorical/qualitative)；注意y轴上的值，划分可能不同。
直方图（Histogram）：
是同一个类别，可以选择任何区间或组距；直方图的形状非常重要；对于直方图来说x轴上的变量是值（numerical/quantitative），是可以量化的。
基础概念
正态分布：
正态分布百度百科 


正态分布峰值：众数；形状大致对称；大多数都在中间位置

偏斜分布：
当多数数据集中在曲线的一端，而少数数据在曲线的另一端，数据分布的形态就产生了偏斜。当偏斜的一边的趋向正数的方向，叫正偏态。当偏斜的一边的趋向负数的方向叫负偏态。

负偏斜分布


小习题
1.图判断

X轴上的数据是什么类型？
A数值型
B类别型
选择 B
图中柱的高度代表相对频率吗？
A是
B不是
选择 B
These are overall percentages. To be relative, they would have to add to 100%.

2.以下哪个更适合用来计算 n ？
A频率表
B直方图
选择 A
以下哪个更适合用来分析分布的形状？
A频率表
B直方图
选择 B

2018-05-14-Google Spreadsheet
execel的使用。

2018-05-14-集中趋势
基础概念
中心测量方法，描述分布中心的情况:
众数：出现频率最高的值或范围。  
中位数：刚好分布在中间的值。
平均值：位于分布中间特定位置的统计值。

众数讨论
众数可用于描述任何数据类型，数值型和类别型都可以。
出现频率最高的值或范围。
不是数据集中的每个数据都影响众数。
众数可能随着样本的不同而不同。
直方图中，众数随着组距的改变而改变。众数与呈现数据的方式有很大关系。
众数没有计算公式。

均匀分布-没有众数。


多峰分布:存在两个或多个明显清晰的趋势的分布。-不只一个众数。


练习：这个分布图的众数是什么？

AMale  BFemale  C1000  D7000
答案 A

平均值讨论
算数平均值(样本平均值)：
总体的均值μ： N为总体的数量
均值特性：
分布中的所有值都影响平均值。
平均值可用公式来描述。
同一个总体中的多个样本会有相似的平均值。
一个样本的平均值可以用来推论其所在的总体。
如果向数据集中添加一个极值，它的平均值会发生改变。

当出现异常数值（outlier）时，平均值会有误导性；异常数值会将平均值拉向异常方向，造成偏斜分布，使得平均值难以具备数据中位数的代表性。

众数不受异常数值影响，而平均值会受到很大影响。

中位数讨论
中位数是位于“中间”的数据，意味着有一半数据值小于它，而另一半大于它。
如何让中位数更加有用？-按顺序排列数据。
如何找出中位数？1.按顺序排列数据，找到最中间的一个。2.按顺序排列数据，如果数据量为偶数的数据集中查找中位数时，求中间2个数字的平均数。
中位数公式：n个值的数据集的中位数,排序后：
1.n为偶数：
2.n为奇数：

在含异常值的数据中统计中位数，发现中位数的变化并不大，中位数的这一趋势非常稳健，稳健即强大且稳定，即使偏离了基准也不会受到很大的影响。

小总结
有些时候由于存在异常数值，均值无法描述分布中心；在有些情况下，众数也无法描述分布中心；中位数不会考虑到所有数据点；在处理高偏斜分布时，中位数通常能更好的反映出集中趋势。

在正向偏斜分布中，如下图：众数&amp;lt;中位数&amp;lt;均值


在正态分布中，如下图：均值=中位数=众数


中心测量方法总结


  
    
       
      有一个简单的公式
      如果数据集中有数据的值变化，它也一定会变化
      不受组距变化的影响
      不易受到异常值的影响
      容易在直方图上找到
    
  
  
    
      mean均值
      V
      V
      V
       
       
    
    
      median中位数
       
       
      V
      V
       
    
    
      mode众数
       
       
       
      V
      V
    
  


小练习

此图中众数1=众数2，均值1=均值2，中间数1=中间数2
众数均为40000-50000
平均值50000
中间数也差不多在中间

2018-05-16-可变性
范围(值域)是观察到的最大值和最小值之间的差。
值域不包括细节信息，因为值域是仅仅在2个数据的基础上得出的。
异常数值增大差异性，值域作为度量标准时，有异常数值，值域不可能准确的代表数据的差异性。
忽略尾部
处理异常数值：处理异常数值的一种方法就是忽略分布中的上尾和下尾。习惯上，统计学家会忽略较低的25%和较高的25%。
虽然建议去除上下各25%的数据点，但我们在去除数据点时还需要特别谨慎，特别是在数据量不大的情况下，去除一半的数据点会让我们丢失大量数据的信息。
一般来说，在去除数据点前，我们建议首先将数据点通过图像表述出来（直方图、散点图、箱线图等），这样可以帮助你获得对数据整理分析的了解。然后，基于项目的背景，判断你更关心数据的哪一部分（大多数正常数据，还是小部分异常数据），因为在一些项目背景下，你可能更关心那些异常值。最后，是基于现有的数据量作出决定，究竟是直接丢弃部分数据还是对部分作出调整，亦或是有保留地接受所有数据。特别记住一点，没有一种分析方法100%正确，但我们总可以尝试根据不同的需求找到一种最合理的方法。
四分位差
找出方法：排序的数据集，从中间分成2半，再找一半的中位数。
第一个四分位数：称为总体的Q1
中位数：Q2
第三个四分位：称为总体的Q3
四分位差(IQR)：Q3-Q1 能表示图的差异性。
几乎50%的数据在IQR间。
IQR不受异常值的影响。 
The Interquartile range (IQR) is the distance be- tween the 1st quartile and 3rd quartile and gives us the range of the middle 50% of our data. The IQR is easily found by computing: Q3-Q1.

异常值
异常值：如果一个值小于Q1减去1.5倍的IQR或者大于Q3加上1.5倍IQR，则这个数就被认为是异常数值。
Lower Outliers &amp;lt; Q1-1.5(IQR)
Upper Outliers &amp;gt; Q3+1.5(IQR)

箱线图-表示中位数、四分位差、最小值和最大值有效。
箱线图（盒须图）：直观的表示四分位数和异常数值。箱线图外面的点表示异常值，在此上下文中，“min”和“max”是指样本中不包括异常值在内的最小值和最大值。箱线图的方向可以有不同。
  
箱线图的对比，箱线图与直方图连线对应：

值域和IQR都无法将所有数据集考虑进来，完全不相同的两个数据集也可能有相同的IQR，不同的图可能有相同的箱线图。


如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©商业转载请联系作者获得授权，非商业转载请注明出处。
</content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> 机器学习 </tag>
        
          <tag> 描述统计学 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Kerberos具体实践1-Kerberos环境准备及安装</title>
      <url>/bigdata/2018/05/05/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B51-Kerberos%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%E5%8F%8A%E5%AE%89%E8%A3%85/</url>
      <content type="text">
  本文属于Kerberos具体实践整理的第一部分，主要涉及到Kerberos集群的安装以及基本命令的使用。


概述
为什么要将HDFS和Kerberos整合
HDFS权限控制过于简单，直接使用hadoop的acl进行权限控制，可能会出现在程序中使用System.setProperty(“HADOOP_USER_NAME”,”root”);直接更改用户，伪装成任意用户使用。因此，需要加强HDFS权限控制，则采用进行kerberos认证的方式。
hadoop的用户鉴权是基于JAAS的，其中hadoop.security.authentication属性有simple 和kerberos 等方式。如果hadoop.security.authentication等于”kerberos”,那么是“hadoop-user-kerberos”或者“hadoop-keytab-kerberos”，否则是“hadoop-simple”。在使用了kerberos的情况下，从javax.security.auth.kerberos.KerberosPrincipal的实例获取username。在没有使用kerberos时，首先读取hadoop的系统环境变量，如果没有的话，对于windows 从com.sun.security.auth.NTUserPrincipal获取用户名，对于类unix 从com.sun.security.auth.UnixPrincipal中获得用户名，然后再看该用户属于哪个group，从而完成登陆认证。
Kerberos原理
术语说明


  
    
      术语
      简述
    
  
  
    
      KDC
      在启用Kerberos的环境中，KDC用于验证各个模块
    
    
      Kerberos KDC Server
      KDC所在的机器
    
    
      Kerberos Client
      任何一个需要通过KDC认证的机器（或模块）
    
    
      Principal
      用于验证一个用户或者一个Service的唯一的标识，相当于一个账号，需要为其设置密码（这个密码也被称之为Key）
    
    
      Keytab
      包含有一个或多个Principal以及其密码的文件
    
    
      Relam
      由KDC以及多个Kerberos Client组成的网络
    
    
      KDC Admin Account
      KDC中拥有管理权限的账户（例如添加、修改、删除Principal）
    
    
      Authentication Server (AS)
      用于初始化认证，并生成Ticket Granting Ticket (TGT)
    
    
      Ticket Granting Server (TGS)
      在TGT的基础上生成Service Ticket。一般情况下AS和TGS都在KDC的Server上
    
  


Kerberos原理则不在此处进行说明，以后再做补充@TODO。本文直接先说明测试部署过程。
部署操作
本文中部署操作首先先进行Kerberos单节点KDC部署，之后将单节点KDC的基础上结合HDFS进行配置，最后对整体集群进行优化，部署主从KDC防止Kerberos单点故障。
全部部署过程均在RHEL 7.1系统上进行。

  参考链接：
Kerberos从入门到放弃（一）：HDFS使用kerberos
HDFS配置Kerberos认证


环境说明
系统环境：
操作系统：RHEL 7.1
Hadoop版本：2.6.0-cdh5.11.0
JDK版本：jdk1.8.0
运行用户：hadoop
Kerberos单节点KDC部署
节点规划:


  
    
      hosts
      角色
    
  
  
    
      node1
      kerberos server、AS、TGS、agent、namenode、datanode、zk
    
    
      node2
      kerberos client、SecondaryNameNode 、datanode、zk
    
    
      node3
      kerberos client、datanode、zk
    
  


(1)添加主机名解析到/etc/hosts文件或配置DNS
在Kerberos中，Principal中需各主机的FQDN名，所以集群中需要有DNS。
但在本次测试中，未进行DNS配置，直接使用配置hosts进行测试。
[root@node1 ~]# cat /etc/hosts
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.211.55.10 node1
10.211.55.11 node2
10.211.55.12 node3


注意：hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。
(2)配置NTP
Kerberos集群对时间同步敏感度较高，默认时间相差超过5分钟就会出现问题，所以最好是在集群中增加NTP服务器。不过我的集群机器比较少，先跳过NTP吧。
(3)安装Kerberos
Kerberos有不同的实现，如MIT KDC、Microsoft Active Directory等。我这里采用的是MIT KDC。
在 KDC (这里是 node1 ) 上安装包 krb5、krb5-server 和 krb5-client：
$ yum install krb5-server krb5-libs krb5-workstation  -y 


在其他节点（node1、node2、node3）安装 krb5-devel、krb5-workstation：
$ yum install krb5-libs krb5-workstation  -y


(4).修改配置文件
kdc 服务器涉及到三个配置文件：
/etc/krb5.conf
/var/kerberos/krb5kdc/kdc.conf
/var/kerberos/krb5kdc/kadm5.acl
a).krb5.conf配置
/etc/krb5.conf:包含Kerberos的配置信息。例如，KDC的位置，Kerberos的admin的realms 等。需要所有使用的Kerberos的机器上的配置文件都同步。这里仅列举需要的基本配置。详细参考krb5.conf

[root@node1 etc]# cat krb5.conf
# Configuration snippets may be placed in this directory as well
includedir /etc/krb5.conf.d/
[logging] #[logging]：表示 server 端的日志的打印位置
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults] #[libdefaults]：每种连接的默认配置，需要注意以下几个关键的小配置
 default_realm = HADOOP.COM #设置Kerberos应用程序的默认领域。如果您有多个领域，只需向[realms]节添加其他的语句
 dns_lookup_realm = false
 #clockskew = 120 #时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。~~
 ticket_lifetime = 24h #表明凭证生效的时限，一般为24小时
 renew_lifetime = 7d #表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败
 forwardable = true #允许转发解析请求  
 rdns = false
 udp_preference_limit = 1 #禁止使用udp可以防止一个Hadoop中的错误

[realms] #列举使用的realm
HADOOP.COM = {
  kdc = node1:88 #代表要kdc的位置。格式是机器:端口。测试过程中也可不加端口。
  admin_server = node1:749 #代表admin的位置。格式是机器:端口。测试过程中也可不加端口。
  default_domain = HADOOP.COM #代表默认的域名。
 }

[kdc]
 profile=/var/kerberos/krb5kdc/kdc.conf
 
#[domain_realm] 
#.hadoop.com = HADOOP.COM
#hadoop.com = HADOOP.COM


总结：

  这个文件可以用include和includedir引入其他文件或文件夹，但必须是绝对路径
  realms:包含kerberos realm的名字（部分键控），显示关于realm的特殊信息，包括该realm内的主机要从哪里寻找kerberos的server。
  domain realm：将domain名和子domain名映射到realm名上
  必须填的有以下几项：
  default-realm：在libdefault部分
  admin_server：在realm部分
  domain_realm：当domain名和realm名不同的时候要设置
  logging：当该机器作为KDC时要设置
  [appdefaults]：可以设定一些针对特定应用的配置，覆盖默认配置。


b).kdc.conf配置
/var/kerberos/krb5kdc/kdc.conf:包括KDC的配置信息。默认放在 /usr/local/var/krb5kdc。或者通过覆盖KRB5_KDC_PROFILE环境变量修改配置文件位置。详细参考kdc.conf。
[root@node1 ~]# cat /var/kerberos/krb5kdc/kdc.conf
[kdcdefaults]
 kdc_ports = 88
 kdc_tcp_ports = 88

[realms]
 HADOOP.COM = { #是设定的 realms。名字随意。Kerberos 可以支持多个 realms，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个realms跟机器的host没有大关系。
  #master_key_type = aes256-cts
  #和supported_enctypes默认使用aes256-cts。由于，JAVA使用aes256-cts验证方式需要安装额外的jar包（后面再做说明）。推荐不使用，并且删除aes256-cts。
  kadmind_port = 749
  acl_file = /var/kerberos/krb5kdc/kadm5.acl #标注了admin的用户权限，需要用户自己创建。文件格式是：Kerberos_principal permissions [target_principal] [restrictions] 支持通配符等。最简单的写法是*/admin@HADOOP.COM *,代表名称匹配*/admin@HADOOP.COM 都认为是admin，权限是 *。代表全部权限。
  dict_file = /usr/share/dict/words
  database_name = /var/kerberos/krb5kdc/principal
  key_stash_file =  /var/kerberos/krb5kdc/.k5.HADOOP.COM
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab #KDC 进行校验的 keytab
  max_life = 24h
  max_renewable_life = 10d #涉及到是否能进行ticket的renwe必须配置
  default_principal_flags = +renewable, +forwardable
  supported_enctypes = des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal #支持的校验方式.注意把aes256-cts去掉
 }


总结：

  HADOOP.COM： 是设定的 realms。名字随意。Kerberos可以支持多个。
  realms，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 realms 跟机器的 host 没有大关系。
  master_key_type：和 supported_enctypes 默认使用 aes256-cts。JAVA 使用 aes256-cts 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用aes256-cts 算法，这样就不需要安装 JCE 。
  acl_file：标注了 admin 的用户权限，需要用户自己创建。文件格式是：Kerberos_principal permissions [target_principal] [restrictions]
  supported_enctypes：支持的校验方式。
  admin_keytab：KDC 进行校验的 keytab。



  补充说明-关于AES-256加密（未测试，参考HDFS配置Kerberos认证）： 
对于使用 centos5. 6 及以上的系统，默认使用 AES-256 来加密的。这就需要集群中的所有节点上安装 JCE，如果你使用的是 JDK1.6 ，则到 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 6 页面下载，如果是 JDK1.7，则到 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 7 下载。下载的文件是一个 zip 包，解开后，将里面的两个文件放到下面的目录中：$JAVA_HOME/jre/lib/security


c).kadm5.acl配置
为了能够不直接访问KDC控制台而从Kerberos数据库添加和删除主体，请对Kerberos管理服务器指示允许哪些主体执行哪些操作。通过编辑文件 /var/kerberos/krb5kdc/kadm5.acl完成此操作。ACL（访问控制列表）允许您精确指定特权。
[root@node1 ~]# cat /var/kerberos/krb5kdc/kadm5.acl
*/admin@HADOOP.COM	*



(5)同步配置文件
将 kdc 中的 /etc/krb5.conf 拷贝到集群中其他服务器即可。
$ scp /etc/krb5.conf node2:/etc/krb5.conf
$ scp /etc/krb5.conf node3:/etc/krb5.conf


请确认集群如果关闭了 selinux。

(6)创建数据库
在node1上运行初始化数据库命令。其中 -r 指定对应 realm。
该命令会在 /var/kerberos/krb5kdc/ 目录下创建 principal 数据库。
如果遇到数据库已经存在的提示，可以把/var/kerberos/krb5kdc/目录下的principal的相关文件都删除掉。默认的数据库名字都是 principal。可以使用 -d 指定数据库名字。
$ kdb5_util create -r JAVACHEN.COM -s



  补充小技巧-（未测试，其他帖子看到的）：出现Loading random data的时候另开个终端执行点消耗CPU的命令如cat /dev/sda &amp;gt; /dev/urandom 可以加快随机数采集。


操作结果小例子：
[root@node1 etc]# kdb5_util create -r HADOOP.COM -s
Loading random data
Initializing database '/var/kerberos/krb5kdc/principal' for realm 'HADOOP.COM',
master key name 'K/M@HADOOP.COM'
You will be prompted for the database Master Password.    #在此处输入的是root@1234
It is important that you NOT FORGET this password.
Enter KDC database master key:
Re-enter KDC database master key to verify:



(7)启动服务
在node1节点上运行：
[root@node1 etc]# service krb5kdc start
Redirecting to /bin/systemctl start krb5kdc.service
[root@node1 etc]# service kadmin start
Redirecting to /bin/systemctl start kadmin.service


至此kerberos，搭建完毕。

Kerberos基本环境创建及简单命令测试
(1)创建Kerberos管理员
关于 kerberos 的管理，可以使用 kadmin.local 或 kadmin，至于使用哪个，取决于账户和访问权限：

  如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local
  如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin


在node1上创建远程管理的管理员，系统会提示输入密码，密码不能为空，且需妥善保存。：
#手动输入两次密码，这里密码为 root
$ kadmin.local -q &quot;addprinc root/admin&quot;
# 也可以不用手动输入密码，通过echo将密码引入
$ echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;


建议：
最好把username设为root，会提示输入密码，可以输入一个密码。
创建第一个principal，必须在KDC自身的终端上进行，而且需要以root登录，这样才可以执行kadmin.local命令。

操作结果小例子：
[root@node1 etc]# kadmin.local -q &quot;addprinc root/admin&quot;
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for root/admin@HADOOP.COM; defaulting to no policy
Enter password for principal &quot;root/admin@HADOOP.COM&quot;:
Re-enter password for principal &quot;root/admin@HADOOP.COM&quot;:
Principal &quot;root/admin@HADOOP.COM&quot; created.



(2)添加更多主体-创建host主体并将自己的host主题添加到自己密钥表文件
a).基于Kerberos 的应用程序(例如 kprop)将使用主机主体将变更传播到从KDC服务器。也可以通过该主体提供对使用应用程序(如ssh)的KDC服务器的安全远程访问。
请注意，当主体实例为主机名时，无论名称服务中的域名是大写还是小写，都必须以小写字母指定FQDN。

kadmin: addprinc -randkey host/node1
Principal &quot;host/node1@HADOOP.COM&quot; created. 
kadmin:


或用命令（创建了node1、node2、node3的host主体）：
kadmin.local -q &quot;addprinc -randkey host/node1@HADOOP.COM&quot;
kadmin.local -q &quot;addprinc -randkey host/node2@HADOOP.COM&quot;
kadmin.local -q &quot;addprinc -randkey host/node3@HADOOP.COM&quot;


b).将KDC服务器的host主体添加到KDC服务器的密钥表文件。
通过将主机主体添加到密钥表文件，允许应用服务器(如sshd)自动使用该主体。
注意：将自己的host主体添加到自己的密钥表文件。

#在node1上执行
kadmin: ktadd host/node1



(3)Kerberos简单命令测试
输入kadmin或者kadmin.local命令进入kerberos的shell(登录到管理员账户:如果在本机上，可以通过kadmin.local直接登录。其它机器的，先使用kinit进行验证)，如下：
1、其他机器上先使用kinit进行验证：
echo root@1234|kinit root/admin  
2、之后再使用kadmin或者kadmin.local命令进入kerberos的shell


a).principals操作
查看principals
$ kadmin: list_principals


添加一个新的 principal
$ kadmin:  addprinc user1  


删除 principal
$ kadmin:  delprinc user1  
$ kadmin: exit


也可以直接通过下面的命令来执行：
#提示需要输入密码
$ kadmin -p root/admin -q &quot;list_principals&quot;
$ kadmin -p root/admin -q &quot;addprinc user2&quot;
$ kadmin -p root/admin -q &quot;delprinc user2&quot;

#不用输入密码
$ kadmin.local -q &quot;list_principals&quot;
$ kadmin.local -q &quot;addprinc user2&quot;
$ kadmin.local -q &quot;delprinc user2&quot;



b).ticket操作:创建一个测试用户test，密码设置为test

$ echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;


获取 test 用户的 ticket：
#通过用户名和密码进行登录
[root@node2 ~]# kinit test
Password for test@HADOOP.COM:
[root@node2 ~]# klist  -e
Ticket cache: KEYRING:persistent:0:krb_ccache_CGo77JQ
Default principal: test@HADOOP.COM
Valid starting       Expires              Service principal
09/06/2017 16:06:57  09/07/2017 16:06:57  krbtgt/HADOOP.COM@HADOOP.COM
	renew until 09/13/2017 16:06:57, Etype (skey, tkt): des3-cbc-sha1, des3-cbc-sha1


销毁该 test 用户的 ticket:
[root@node2 ~]# kdestroy
Other credential caches present, use -A to destroy all
[root@node2 ~]# kdestroy -A
[root@node2 ~]# klist  -e
klist: Credentials cache keyring 'persistent:0:krb_ccache_CGo77JQ' not found
[root@node2 ~]# klist
klist: Credentials cache keyring 'persistent:0:krb_ccache_CGo77JQ' not found


更新 ticket:
$ kinit root/admin
  Password for root/admin@HADOOP.COM:
$  klist
  Ticket cache: FILE:/tmp/krb5cc_0
  Default principal: root/admin@JAVACHEN.COM
  Valid starting     Expires            Service principal
  11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JAVACHEN.COM@JAVACHEN.COM
    renew until 11/17/14 15:33:57
  Kerberos 4 ticket cache: /tmp/tkt0
  klist: You have no tickets cached
$ kinit -R
$ klist
  Ticket cache: FILE:/tmp/krb5cc_0
  Default principal: root/admin@JAVACHEN.COM
  Valid starting     Expires            Service principal
  11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JAVACHEN.COM@JAVACHEN.COM
    renew until 11/17/14 15:33:57
  Kerberos 4 ticket cache: /tmp/tkt0
  klist: You have no tickets cached


c). 抽取密钥到在本地keytab文件
抽取密钥并将其储存在本地keytab文件/etc/krb5.keytab中。这个文件由超级用户拥有，所以您必须是root用户才能在kadmin shell 中执行以下命令:
$ kadmin.local -q &quot;ktadd kadmin/admin&quot;
$ klist -k /etc/krb5.keytab
  Keytab name: FILE:/etc/krb5.keytab
  KVNO Principal
  ---- ----------------------------------------------------------------------
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
   -----------------



至此，本篇内容完成。以上为Kerberos单KDC版搭建过程，下篇文章进行HDFS和Kerberos整合操作的整理。
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©商业转载请联系作者获得授权，非商业转载请注明出处。

</content>
      <categories>
        
          <category> BigData </category>
        
      </categories>
      <tags>
        
          <tag> Kerberos </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>SparkStreaming程序中checkpoint与广播变量兼容处理</title>
      <url>/bigdata/2018/04/04/SparkStreaming%E7%A8%8B%E5%BA%8F%E4%B8%ADcheckpoint%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%85%BC%E5%AE%B9%E5%A4%84%E7%90%86/</url>
      <content type="text">
  上文中说明了如何使用kafka连接池来优化程序，但是在上文中预留了一个问题，就是当使用上文的方式下发KafkaPool广播变量时，如果Spark Streaming程序中也使用了checkpoint，则如果程序中断而重启程序，广播变量无法从checkpoint中恢复，会出现“java.lang.ClassCastException:B cannot be cast to KafkaPool”问题，所以在此篇文章中，对此问题进行解决。
注意: 本文中使用的版本是spark2.2.1和kafka0.10.1.1


背景
在spark信令处理程序中使用checkpoint主要是因为从源头读取kafka数据的时候记录offset，防止数据丢失；并且目前是做的是容器化的集群，如果集群down了，会自动重启容器并且也能把程序恢复。
不过对于Spark Streaming中防止数据丢失可以有两种方式:

  使用Spark Streaming的checkpoint机制
  自己维护kafka offset


但是，有人说checkpoint有弊端，并且我也遇到了序列化的这个问题。在查资料的过程中，看到有评论说checkpoint与广播变量就是不能同时使用（这是不对的），所以也思考过要不要改成自己手动维护offset，而且发现有好多人也这么做了。不过我们代码更新迭代并不频繁，不会被checkpoint影响太大，所以还是决定再试试使用checkpoint，终于好不容易也让我找到了解决方法，真是巨开心,下面就说下怎么解决的。

解决-示例说明
参考例子
在Spark Streaming中，目前为止累加器和广播变量确实是无法从checkpoint恢复的。但是如果在程序中既使用到checkpoint又使用了累加器和广播变量的话，最好对累加器和广播变量做懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。
解决方法其实就在spark官方的项目的examples中，访问请戳: RecoverableNetworkWordCount ，它是广播变量和累加器与checkpoint兼容的一个例子。下面我就把代码摘出来记录一下。
第一步：用单例模式来获取或生成广播变量和累加器
/**
 * Use this singleton to get or register a Broadcast variable.
 * 单例模式获取广播变量wordBlacklist
 */
object WordBlacklist {
  @volatile private var instance: Broadcast[Seq[String]] = null
  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          val wordBlacklist = Seq(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
          instance = sc.broadcast(wordBlacklist)
        }
      }
    }
    instance
  }
}
/**
 * Use this singleton to get or register an Accumulator.
 * 累加器
 */
object DroppedWordsCounter {
  @volatile private var instance: LongAccumulator = null
  def getInstance(sc: SparkContext): LongAccumulator = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          instance = sc.longAccumulator(&quot;WordsInBlacklistCounter&quot;)
        }
      }
    }
    instance
  }
}


第二步：在Spark主程序中使用
在主程序的driver端位置使用，懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。
object RecoverableNetworkWordCount {
  // 这个是用来生成StreamingContext对象的用户自定义的方法
  def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
    : StreamingContext = {
    // If you do not see this printed, that means the StreamingContext has been loaded from the new checkpoint
    // 如果没有打印出这句话，说明是使用检查点元数据恢复一个StreamingContext
    println(&quot;Creating new context&quot;)
    val outputFile = new File(outputPath)
    if (outputFile.exists()) outputFile.delete()
    val sparkConf = new SparkConf().setAppName(&quot;RecoverableNetworkWordCount&quot;)
    // 创建sparkContext，1秒一个批次
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    // 设置checkpoint
    ssc.checkpoint(checkpointDirectory)
    // Create a socket stream on target ip:port and count the words in input stream of \n delimited text (eg. generated by 'nc')
    // 这里是测试的socket stream
    val lines = ssc.socketTextStream(ip, port)
    val words = lines.flatMap(_.split(&quot; &quot;))
    val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
    wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =&amp;gt;
      // Get or register the blacklist Broadcast 广播变量
      val blacklist = WordBlacklist.getInstance(rdd.sparkContext)
      // Get or register the droppedWordsCounter Accumulator 累加器
      val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)
      // Use blacklist to drop words and use droppedWordsCounter to count them 使用广播变量和累加器
      val counts = rdd.filter { case (word, count) =&amp;gt;
        if (blacklist.value.contains(word)) {
          droppedWordsCounter.add(count)
          false
        } else {
          true
        }
      }.collect().mkString(&quot;[&quot;, &quot;, &quot;, &quot;]&quot;)
      val output = s&quot;Counts at time $time $counts&quot;
      println(output)
      println(s&quot;Dropped ${droppedWordsCounter.value} word(s) totally&quot;)
      println(s&quot;Appending to ${outputFile.getAbsolutePath}&quot;)
      Files.append(output + &quot;\n&quot;, outputFile, Charset.defaultCharset())
    }
    ssc
  }
  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println(s&quot;Your arguments were ${args.mkString(&quot;[&quot;, &quot;, &quot;, &quot;]&quot;)}&quot;)
      System.err.println(
        &quot;&quot;&quot;
          |Usage: RecoverableNetworkWordCount &amp;lt;hostname&amp;gt; &amp;lt;port&amp;gt; &amp;lt;checkpoint-directory&amp;gt;
          |     &amp;lt;output-file&amp;gt;. &amp;lt;hostname&amp;gt; and &amp;lt;port&amp;gt; describe the TCP server that Spark
          |     Streaming would connect to receive data. &amp;lt;checkpoint-directory&amp;gt; directory to
          |     HDFS-compatible file system which checkpoint data &amp;lt;output-file&amp;gt; file to which the
          |     word counts will be appended
          |In local mode, &amp;lt;master&amp;gt; should be 'local[n]' with n &amp;gt; 1
          |Both &amp;lt;checkpoint-directory&amp;gt; and &amp;lt;output-file&amp;gt; must be absolute paths
        &quot;&quot;&quot;.stripMargin
      )
      System.exit(1)
    }
    val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args
    // 使用StreamingContext.getOrCreate来创建StreamingContext对象，传入的第一个参数是checkpoint的存放目录，第二参数是生成StreamingContext对象的用户自定义方法。
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () =&amp;gt; createContext(ip, port, outputPath, checkpointDirectory))
    ssc.start()
    ssc.awaitTermination()
  }
}


解决-SparkStreaming+kafkaPool程序修改(兼容checkpoint和广播变量kafkaPool)
上述内容即如何将checkpoint与广播变量或累加器兼容的例子，下面则结合上述例子，对上文(SparkStreaming写数据到Kafka)的程序做修改来解决“java.lang.ClassCastException:B cannot be cast to KafkaPool”的问题。
第一步：包装KafkaProducer-创建Kafka连接池（不变）
对于上文(SparkStreaming写数据到Kafka)中的程序，第一步保持不变，创建class KafkaPool 以及object KafkaPool，将KafkaProducer以lazy val的方式进行包装。
import java.util.concurrent.Future
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, RecordMetadata}
//scala中，类名之后的括号中是构造函数的参数列表，() =&amp;gt;是传值传参，KafkaProducer
class KafkaPool[K, V]( createProducer: () =&amp;gt; KafkaProducer[K,V])  extends Serializable{
  //使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法
  //后续在spark主程序中使用时，将kafkapool广播出去到每个executor里面了，然后到每个executor中，当用到的时候，会实例化一个producer，这样就不会有NotSerializableExceptions的问题了。
  lazy val producer = createProducer()
  def send(topic: String, key: K, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, key, value))
  def send(topic: String, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, value))
}
object KafkaPool{
  import scala.collection.JavaConversions._
  def apply[K, V](config: Map[String, Object]): KafkaPool[K, V] = {
      val createProducerFunc = () =&amp;gt; {
        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
        val producer = new KafkaProducer[K, V](config)
        sys.addShutdownHook {
          //当发送ececutor中的jvm shutdown时，kafka能够将缓冲区的消息发送出去。
          producer.close()
        }
        producer
      }
      new KafkaPool(createProducerFunc)
  }
  def apply[K, V](config: java.util.Properties): KafkaPool[K, V] = apply(config.toMap)
}


第二步：对广播变量懒实例化操作，使用单例模式来获取广播变量KafkaPool
这里较上文(SparkStreaming写数据到Kafka)第一种方式中的直接下发广播变量有所区别，而是创建来一个GetKafkaPoolBroadcast的getKafkaPool方法，用于在主程序中driver端调用此方法时再获取或生成广播变量。
import java.util.Properties
import org.apache.spark.SparkContext
import org.apache.spark.internal.Logging
import org.apache.spark.broadcast.Broadcast

object GetKafkaPoolBroadcast extends Logging {
  @volatile private var kafkapool: Broadcast[KafkaPool[String, String]]  = null
  def getKafkaPool(sc: SparkContext,proKafkaBrokerAddr:String): Broadcast[KafkaPool[String, String]] = {
    if (kafkapool == null) {
      synchronized {
        if (kafkapool == null) {
          val kafkaProducerConfig = {
            val props = new Properties()
            props.put(&quot;metadata.broker.list&quot;,proKafkaBrokerAddr)
            props.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;)
            props.put(&quot;sasl.mechanism&quot;,&quot;PLAIN&quot;)
            props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
            props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
            props.put(&quot;bootstrap.servers&quot;,proKafkaBrokerAddr)
            props
          }
          log.warn(&quot;kafka producer init done!&quot;)
          kafkapool = sc.broadcast(KafkaPool[String, String](kafkaProducerConfig))
        }
      }
    }
    kafkapool
  }
}


第三步：在主程序中使用GetKafkaPoolBroadcast的getKafkaPool获取或生成广播变量
在主程序中，driver端的位置调用此方法，这样可以使广播变量在driver失败重启时能够重新示例化。
    //保存处理后的数据到kafka
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      // driver端运行，涉及操作：广播变量的初始化和更新
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        // Get or register the kafkaPool Broadcast 获取或生成广播变量kafkaPool
        val kafkaProducer: Broadcast[KafkaPool[String, String]]=GetKafkaPoolBroadcast.getKafkaPool(rdd.sparkContext,proKafkaBrokerAddr)
        rdd.foreach(record=&amp;gt;{
          kafkaProducer.value.send(proKafkaTopicName,record)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


这样，就不会再尝试从checkpoint中恢复广播变量，而可以避免“java.lang.ClassCastException:B cannot be cast to KafkaPool”这个问题啦。
至此，本篇内容完成。
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©商业转载请联系作者获得授权，非商业转载请注明出处。
</content>
      <categories>
        
          <category> BigData </category>
        
      </categories>
      <tags>
        
          <tag> Spark </tag>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>SparkStreaming输出数据到Kafka--Kafka连接池的使用</title>
      <url>/bigdata/2018/04/02/SparkStreaming%E5%86%99%E6%95%B0%E6%8D%AE%E5%88%B0Kafka-Kafka%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      <content type="text">
  最近把spark实时流处理的信令处理程序，由原来的向kafka0.8.2.1中写入数据，改成了向带ACL权限认证的kafka0.10.1.1中写入数据，因此在之前的基础上创建连接前会多一个认证过程，因此导致写入效率有些低下，所以使用Kafka连接池来优化之前程序。
注意: 本文中使用的版本是spark2.2.1和kafka0.10.1.1


原写入方式描述
使用Spark Streaming从Kafka中接收数据时，可以使用Spark提供的统一的接口来接收数据，但是写入数据的时候，并没有spark官方提供的写入接口，需要自己写使用底层的kafka方法，使用producer写入。
下面是原写入方式的程序示例:
第一部分：Spark Streaming主程序部分
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        // 不能在这里创建KafkaProducer
	rdd.foreachPartition(iter=&amp;gt;{
          process.writeAsPartitionToKafka(proKafkaTopicName,iter,cacheNum,props)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


在每个partation中调用ProcessData类中的writeAsPartitionToKafka方法来向kafka写入数据。

  特别说明：这里是通过调用ProcessData类中的方法，在ProcessData类中的方法中创建KafkaProducer来向kafka里写入的，如果直接写创建KafkaProducer，不能把将KafkaProducer的创建放在foreachPartition外边，因为KafkaProducer是不可序列化的（not serializable）。


第二部分：主程序在rdd.foreachPartition中调用以下类中的方法把数据写入kafka中
    final class ProcessData extends Logging{
      /**
        * 数据发送
        * @param topic
        * @param messages
        * @param props
        */
      def sendMessages(topic: String, messages: String, props: Properties) {
        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
        val producer: Producer[String, String] = new KafkaProducer[String, String](props)
        try{
          val msg: ProducerRecord[String, String] = new ProducerRecord[String,String](topic,messages)
          producer.send(msg)
        }catch{
          case e: IOException =&amp;gt;
            logError(&quot;partition write data to  kafka exception : &quot; + e.getMessage + &quot;\n&quot;)
        } finally {
          producer.close
        }
      }
      /**
        * 处理数据，按N条写入一次
        * @param topicName
        * @param iter
        * @param cache 缓存数
        * @param props
        * @return
        */
      def writeAsPartitionToKafka(topicName: String,iter: Iterator[String], cache: Int,props:Properties): Try[Unit] = Try {
        var record_sum=&quot;&quot;   //初始化空串
        var count_sum=0     //计数器
        var record=&quot;&quot;
        while (iter.hasNext) {
          record = iter.next()
          record_sum += record+&quot;\n&quot;
          count_sum = count_sum + 1
          if (count_sum == cache) {
            sendMessages(topicName,record_sum,props)
            record_sum = &quot;&quot;
            count_sum = 0
          }
        }
        if (!record_sum.isEmpty) {
          sendMessages(topicName,record_sum,props)
        }
      }
    }


此类中主要2个方法，writeAsPartitionToKafka是用于处理数据，sendMessages用来创建KafkaProducer来向kafka生产消息。
writeAsPartitionToKafka方法为了“提高”写入效率，设置了一个cacheNum值，这个方法会将每个patation中的数据以cacheNum条和并成一条数据来向Kafka中写入，因此是cacheNum条数据以“\n”来分割合并成一条在Kafka里的消息，所以对下游数据处理也造成了麻烦。
并且使用此方式，相当于对于每个partation的每cacheNum条记录（即每次调用sendMessages方法，发送1条kafka消息）都需要创建KafkaProducer，然后再利用producer进行输出操作
。还是需要较频繁的建立连接，因此使用kafka连接池来更改程序。

新写入方式描述:使用Kafka连接池更改程序
第一步：包装KafkaProducer-创建Kafka连接池
创建class KafkaPool 以及object KafkaPool，将KafkaProducer以lazy val的方式进行包装。
    import java.util.concurrent.Future
    import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, RecordMetadata}
    //scala中，类名之后的括号中是构造函数的参数列表，() =&amp;gt;是传值传参，KafkaProducer
    class KafkaPool[K, V]( createProducer: () =&amp;gt; KafkaProducer[K,V])  extends Serializable{
      //使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法
      //后续在spark主程序中使用时，将kafkapool广播出去到每个executor里面了，然后到每个executor中，当用到的时候，会实例化一个producer，这样就不会有NotSerializableExceptions的问题了。
      lazy val producer = createProducer()
      def send(topic: String, key: K, value: V): Future[RecordMetadata] = 
        producer.send(new ProducerRecord[K, V](topic, key, value))
      def send(topic: String, value: V): Future[RecordMetadata] = 
        producer.send(new ProducerRecord[K, V](topic, value))
    }
    object KafkaPool{
      import scala.collection.JavaConversions._
      def apply[K, V](config: Map[String, Object]): KafkaPool[K, V] = {
          val createProducerFunc = () =&amp;gt; {
            //kafka权限认证
            System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
            val producer = new KafkaProducer[K, V](config)
            sys.addShutdownHook {
	      //当发送ececutor中的jvm shutdown时，kafka能够将缓冲区的消息发送出去。
              producer.close()
            }
            producer
          }
          new KafkaPool(createProducerFunc)
      }
      def apply[K, V](config: java.util.Properties): KafkaPool[K, V] = apply(config.toMap)
    }



  补充说明：
传值传参和传名的区别（()=&amp;gt;和:=&amp;gt;的区别）
  
    传值
        def test1(code: ()=&amp;gt;Unit){
    println(&quot;start&quot;)
    code() //要想调用传入的代码块，必须写成code()，否则不会调用。  
    println(&quot;end&quot;)
  }
  test1 {//此代码块，传入后立即执行。  
    println(&quot;when evaluated&quot;) //传入就打印
    ()=&amp;gt;{println(&quot;bb&quot;)} // 执行code()才打印
  }

      
      结果: 
when evaluated 
start
bb
end
    
    传名
        def test2(code : =&amp;gt; Unit){  
    println(&quot;start&quot;)  
    code // 这行才会调用传入的代码块，写成code()亦可  
    println(&quot;end&quot;)  
  }  
  test2{// 此处的代码块不会马上被调用  
    println(&quot;when evaluated&quot;)  //执行code的时候才调用
    println(&quot;bb&quot;) //执行code的时候才调用  
  }

      
      结果:
start
when evaluated
bb
end
    
  


第二步：利用广播变量下发KafkaProducer
利用广播变量，给每一个executor自己的KafkaProducer，将KafkaProducer广播到每一个executor中。  
注意：这里暂留一个问题，此种方式只可以Spark Streaming程序不用checkpoint的时候使用，否则，如果程序中断而重启程序，广播变量无法从checkpoint中恢复，会出现 “java.lang.ClassCastException:B cannot be cast to KafkaPool” 的问题，具体解决方式见下篇文章(SparkStreaming程序中checkpoint与广播变量兼容处理)。现在先说明这种不用checkpoint的方式。
在spark主程序中加入如下代码：
    //利用广播变量的形式，将后端写入KafkaProducer广播到每一个executor 注意：这里写广播变量的话，与checkpoint一起用会有问题
    val kafkaProducer: Broadcast[KafkaPool[String, String]] = {
      val kafkaProducerConfig = {
        val props = new Properties()
        props.put(&quot;metadata.broker.list&quot;,proKafkaBrokerAddr)
        props.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;)
        props.put(&quot;sasl.mechanism&quot;,&quot;PLAIN&quot;)
        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
        props.put(&quot;bootstrap.servers&quot;,proKafkaBrokerAddr)
        props
      }
      log.warn(&quot;kafka producer init done!&quot;)
      ssc.sparkContext.broadcast(KafkaPool[String, String](kafkaProducerConfig))
    }


第三步：使用广播变量
spark 主程序中，在每个executor中使用广播变量。
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        rdd.foreach(record=&amp;gt;{
           kafkaProducer.value.send(proKafkaTopicName,record)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


至此，更改完毕。
结果对比
使用Kafka连接池更改程序之前以及之后的处理速度对比如下图所示，写入90W条数据由原来的17953ms变为了1966ms，效率大大提高。


内容即以上，会在下篇文章(SparkStreaming程序中checkpoint与广播变量兼容处理)解决spark streaming中checkpoint和广播变量使用冲突的问题，敬请期待。
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©商业转载请联系作者获得授权，非商业转载请注明出处。
</content>
      <categories>
        
          <category> BigData </category>
        
      </categories>
      <tags>
        
          <tag> Spark </tag>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
