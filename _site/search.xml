<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title>SparkStreaming程序中checkpoint与广播变量兼容处理</title>
      <url>/bigdata/2018/04/04/SparkStreaming%E7%A8%8B%E5%BA%8F%E4%B8%ADcheckpoint%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%85%BC%E5%AE%B9%E5%A4%84%E7%90%86/</url>
      <content type="text">
  上文中说明了如何使用kafka连接池来优化程序，但是在上文中预留了一个问题，就是当使用上文的方式下发KafkaPool广播变量时，如果Spark Streaming程序中也使用了checkpoint，则如果程序中断而重启程序，广播变量无法从checkpoint中恢复，会出现“java.lang.ClassCastException:B cannot be cast to KafkaPool”问题，所以在此篇文章中，对此问题进行解决。
注意: 本文中使用的版本是spark2.2.1和kafka0.10.1.1


背景
在spark信令处理程序中使用checkpoint主要是因为从源头读取kafka数据的时候记录offset，防止数据丢失；并且目前是做的是容器化的集群，如果集群down了，会自动重启容器并且也能把程序恢复。
不过对于Spark Streaming中防止数据丢失可以有两种方式:

  使用Spark Streaming的checkpoint机制
  自己维护kafka offset


但是，有人说checkpoint有弊端，并且我也遇到了序列化的这个问题。在查资料的过程中，看到有评论说checkpoint与广播变量就是不能同时使用（这是不对的），所以也思考过要不要改成自己手动维护offset，而且发现有好多人也这么做了。不过我们代码更新迭代并不频繁，不会被checkpoint影响太大，所以还是决定再试试使用checkpoint，终于好不容易也让我找到了解决方法，真是巨开心,下面就说下怎么解决的。

解决-示例说明
参考例子
在Spark Streaming中，目前为止累加器和广播变量确实是无法从checkpoint恢复的。但是如果在程序中既使用到checkpoint又使用了累加器和广播变量的话，最好对累加器和广播变量做懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。
解决方法其实就在spark官方的项目的examples中，访问请戳: RecoverableNetworkWordCount ，它是广播变量和累加器与checkpoint兼容的一个例子。下面我就把代码摘出来记录一下。
第一步：用单例模式来获取或生成广播变量和累加器
/**
 * Use this singleton to get or register a Broadcast variable.
 * 单例模式获取广播变量wordBlacklist
 */
object WordBlacklist {
  @volatile private var instance: Broadcast[Seq[String]] = null
  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          val wordBlacklist = Seq(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
          instance = sc.broadcast(wordBlacklist)
        }
      }
    }
    instance
  }
}
/**
 * Use this singleton to get or register an Accumulator.
 * 累加器
 */
object DroppedWordsCounter {
  @volatile private var instance: LongAccumulator = null
  def getInstance(sc: SparkContext): LongAccumulator = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          instance = sc.longAccumulator(&quot;WordsInBlacklistCounter&quot;)
        }
      }
    }
    instance
  }
}


第二步：在Spark主程序中使用
在主程序的driver端位置使用，懒实例化操作，这样可以使累加器和广播变量在driver失败重启时能够重新实例化。
object RecoverableNetworkWordCount {
  // 这个是用来生成StreamingContext对象的用户自定义的方法
  def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
    : StreamingContext = {
    // If you do not see this printed, that means the StreamingContext has been loaded from the new checkpoint
    // 如果没有打印出这句话，说明是使用检查点元数据恢复一个StreamingContext
    println(&quot;Creating new context&quot;)
    val outputFile = new File(outputPath)
    if (outputFile.exists()) outputFile.delete()
    val sparkConf = new SparkConf().setAppName(&quot;RecoverableNetworkWordCount&quot;)
    // 创建sparkContext，1秒一个批次
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    // 设置checkpoint
    ssc.checkpoint(checkpointDirectory)
    // Create a socket stream on target ip:port and count the words in input stream of \n delimited text (eg. generated by 'nc')
    // 这里是测试的socket stream
    val lines = ssc.socketTextStream(ip, port)
    val words = lines.flatMap(_.split(&quot; &quot;))
    val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
    wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =&amp;gt;
      // Get or register the blacklist Broadcast 广播变量
      val blacklist = WordBlacklist.getInstance(rdd.sparkContext)
      // Get or register the droppedWordsCounter Accumulator 累加器
      val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)
      // Use blacklist to drop words and use droppedWordsCounter to count them 使用广播变量和累加器
      val counts = rdd.filter { case (word, count) =&amp;gt;
        if (blacklist.value.contains(word)) {
          droppedWordsCounter.add(count)
          false
        } else {
          true
        }
      }.collect().mkString(&quot;[&quot;, &quot;, &quot;, &quot;]&quot;)
      val output = s&quot;Counts at time $time $counts&quot;
      println(output)
      println(s&quot;Dropped ${droppedWordsCounter.value} word(s) totally&quot;)
      println(s&quot;Appending to ${outputFile.getAbsolutePath}&quot;)
      Files.append(output + &quot;\n&quot;, outputFile, Charset.defaultCharset())
    }
    ssc
  }
  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println(s&quot;Your arguments were ${args.mkString(&quot;[&quot;, &quot;, &quot;, &quot;]&quot;)}&quot;)
      System.err.println(
        &quot;&quot;&quot;
          |Usage: RecoverableNetworkWordCount &amp;lt;hostname&amp;gt; &amp;lt;port&amp;gt; &amp;lt;checkpoint-directory&amp;gt;
          |     &amp;lt;output-file&amp;gt;. &amp;lt;hostname&amp;gt; and &amp;lt;port&amp;gt; describe the TCP server that Spark
          |     Streaming would connect to receive data. &amp;lt;checkpoint-directory&amp;gt; directory to
          |     HDFS-compatible file system which checkpoint data &amp;lt;output-file&amp;gt; file to which the
          |     word counts will be appended
          |In local mode, &amp;lt;master&amp;gt; should be 'local[n]' with n &amp;gt; 1
          |Both &amp;lt;checkpoint-directory&amp;gt; and &amp;lt;output-file&amp;gt; must be absolute paths
        &quot;&quot;&quot;.stripMargin
      )
      System.exit(1)
    }
    val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args
    // 使用StreamingContext.getOrCreate来创建StreamingContext对象，传入的第一个参数是checkpoint的存放目录，第二参数是生成StreamingContext对象的用户自定义方法。
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () =&amp;gt; createContext(ip, port, outputPath, checkpointDirectory))
    ssc.start()
    ssc.awaitTermination()
  }
}


解决-SparkStreaming+kafkaPool程序修改(兼容checkpoint和广播变量kafkaPool)
上述内容即如何将checkpoint与广播变量或累加器兼容的例子，下面则结合上述例子，对上文(SparkStreaming写数据到Kafka)的程序做修改来解决“java.lang.ClassCastException:B cannot be cast to KafkaPool”的问题。
第一步：包装KafkaProducer-创建Kafka连接池（不变）
对于上文(SparkStreaming写数据到Kafka)中的程序，第一步保持不变，创建class KafkaPool 以及object KafkaPool，将KafkaProducer以lazy val的方式进行包装。
import java.util.concurrent.Future
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, RecordMetadata}
//scala中，类名之后的括号中是构造函数的参数列表，() =&amp;gt;是传值传参，KafkaProducer
class KafkaPool[K, V]( createProducer: () =&amp;gt; KafkaProducer[K,V])  extends Serializable{
  //使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法
  //后续在spark主程序中使用时，将kafkapool广播出去到每个executor里面了，然后到每个executor中，当用到的时候，会实例化一个producer，这样就不会有NotSerializableExceptions的问题了。
  lazy val producer = createProducer()
  def send(topic: String, key: K, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, key, value))
  def send(topic: String, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, value))
}
object KafkaPool{
  import scala.collection.JavaConversions._
  def apply[K, V](config: Map[String, Object]): KafkaPool[K, V] = {
      val createProducerFunc = () =&amp;gt; {
        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
        val producer = new KafkaProducer[K, V](config)
        sys.addShutdownHook {
          //当发送ececutor中的jvm shutdown时，kafka能够将缓冲区的消息发送出去。
          producer.close()
        }
        producer
      }
      new KafkaPool(createProducerFunc)
  }
  def apply[K, V](config: java.util.Properties): KafkaPool[K, V] = apply(config.toMap)
}


第二步：对广播变量懒实例化操作，使用单例模式来获取广播变量KafkaPool
这里较上文(SparkStreaming写数据到Kafka)第一种方式中的直接下发广播变量有所区别，而是创建来一个GetKafkaPoolBroadcast的getKafkaPool方法，用于在主程序中driver端调用此方法时再获取或生成广播变量。
import java.util.Properties
import org.apache.spark.SparkContext
import org.apache.spark.internal.Logging
import org.apache.spark.broadcast.Broadcast

object GetKafkaPoolBroadcast extends Logging {
  @volatile private var kafkapool: Broadcast[KafkaPool[String, String]]  = null
  def getKafkaPool(sc: SparkContext,proKafkaBrokerAddr:String): Broadcast[KafkaPool[String, String]] = {
    if (kafkapool == null) {
      synchronized {
        if (kafkapool == null) {
          val kafkaProducerConfig = {
            val props = new Properties()
            props.put(&quot;metadata.broker.list&quot;,proKafkaBrokerAddr)
            props.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;)
            props.put(&quot;sasl.mechanism&quot;,&quot;PLAIN&quot;)
            props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
            props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
            props.put(&quot;bootstrap.servers&quot;,proKafkaBrokerAddr)
            props
          }
          log.warn(&quot;kafka producer init done!&quot;)
          kafkapool = sc.broadcast(KafkaPool[String, String](kafkaProducerConfig))
        }
      }
    }
    kafkapool
  }
}


第三步：在主程序中使用GetKafkaPoolBroadcast的getKafkaPool获取或生成广播变量
在主程序中，driver端的位置调用此方法，这样可以使广播变量在driver失败重启时能够重新示例化。
    //保存处理后的数据到kafka
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      // driver端运行，涉及操作：广播变量的初始化和更新
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        // Get or register the kafkaPool Broadcast 获取或生成广播变量kafkaPool
        val kafkaProducer: Broadcast[KafkaPool[String, String]]=GetKafkaPoolBroadcast.getKafkaPool(rdd.sparkContext,proKafkaBrokerAddr)
        rdd.foreach(record=&amp;gt;{
          kafkaProducer.value.send(proKafkaTopicName,record)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


这样，就不会再尝试从checkpoint中恢复广播变量，而可以避免“java.lang.ClassCastException:B cannot be cast to KafkaPool”这个问题啦。
至此，本篇内容完成。
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©转载请注明
</content>
      <categories>
        
          <category> BigData </category>
        
      </categories>
      <tags>
        
          <tag> Spark </tag>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>SparkStreaming输出数据到Kafka--Kafka连接池的使用</title>
      <url>/bigdata/2018/04/02/SparkStreaming%E5%86%99%E6%95%B0%E6%8D%AE%E5%88%B0Kafka-Kafka%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      <content type="text">
  最近把spark实时流处理的信令处理程序，由原来的向kafka0.8.2.1中写入数据，改成了向带ACL权限认证的kafka0.10.1.1中写入数据，因此在之前的基础上创建连接前会多一个认证过程，因此导致写入效率有些低下，所以使用Kafka连接池来优化之前程序。
注意: 本文中使用的版本是spark2.2.1和kafka0.10.1.1


原写入方式描述
使用Spark Streaming从Kafka中接收数据时，可以使用Spark提供的统一的接口来接收数据，但是写入数据的时候，并没有spark官方提供的写入接口，需要自己写使用底层的kafka方法，使用producer写入。
下面是原写入方式的程序示例:
第一部分：Spark Streaming主程序部分
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        // 不能在这里创建KafkaProducer
	rdd.foreachPartition(iter=&amp;gt;{
          process.writeAsPartitionToKafka(proKafkaTopicName,iter,cacheNum,props)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


在每个partation中调用ProcessData类中的writeAsPartitionToKafka方法来向kafka写入数据。

  特别说明：这里是通过调用ProcessData类中的方法，在ProcessData类中的方法中创建KafkaProducer来向kafka里写入的，如果直接写创建KafkaProducer，不能把将KafkaProducer的创建放在foreachPartition外边，因为KafkaProducer是不可序列化的（not serializable）。


第二部分：主程序在rdd.foreachPartition中调用以下类中的方法把数据写入kafka中
    final class ProcessData extends Logging{
      /**
        * 数据发送
        * @param topic
        * @param messages
        * @param props
        */
      def sendMessages(topic: String, messages: String, props: Properties) {
        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
        val producer: Producer[String, String] = new KafkaProducer[String, String](props)
        try{
          val msg: ProducerRecord[String, String] = new ProducerRecord[String,String](topic,messages)
          producer.send(msg)
        }catch{
          case e: IOException =&amp;gt;
            logError(&quot;partition write data to  kafka exception : &quot; + e.getMessage + &quot;\n&quot;)
        } finally {
          producer.close
        }
      }
      /**
        * 处理数据，按N条写入一次
        * @param topicName
        * @param iter
        * @param cache 缓存数
        * @param props
        * @return
        */
      def writeAsPartitionToKafka(topicName: String,iter: Iterator[String], cache: Int,props:Properties): Try[Unit] = Try {
        var record_sum=&quot;&quot;   //初始化空串
        var count_sum=0     //计数器
        var record=&quot;&quot;
        while (iter.hasNext) {
          record = iter.next()
          record_sum += record+&quot;\n&quot;
          count_sum = count_sum + 1
          if (count_sum == cache) {
            sendMessages(topicName,record_sum,props)
            record_sum = &quot;&quot;
            count_sum = 0
          }
        }
        if (!record_sum.isEmpty) {
          sendMessages(topicName,record_sum,props)
        }
      }
    }


此类中主要2个方法，writeAsPartitionToKafka是用于处理数据，sendMessages用来创建KafkaProducer来向kafka生产消息。
writeAsPartitionToKafka方法为了“提高”写入效率，设置了一个cacheNum值，这个方法会将每个patation中的数据以cacheNum条和并成一条数据来向Kafka中写入，因此是cacheNum条数据以“\n”来分割合并成一条在Kafka里的消息，所以对下游数据处理也造成了麻烦。
并且使用此方式，相当于对于每个partation的每cacheNum条记录（即每次调用sendMessages方法，发送1条kafka消息）都需要创建KafkaProducer，然后再利用producer进行输出操作
。还是需要较频繁的建立连接，因此使用kafka连接池来更改程序。

新写入方式描述:使用Kafka连接池更改程序
第一步：包装KafkaProducer-创建Kafka连接池
创建class KafkaPool 以及object KafkaPool，将KafkaProducer以lazy val的方式进行包装。
    import java.util.concurrent.Future
    import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, RecordMetadata}
    //scala中，类名之后的括号中是构造函数的参数列表，() =&amp;gt;是传值传参，KafkaProducer
    class KafkaPool[K, V]( createProducer: () =&amp;gt; KafkaProducer[K,V])  extends Serializable{
      //使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法
      //后续在spark主程序中使用时，将kafkapool广播出去到每个executor里面了，然后到每个executor中，当用到的时候，会实例化一个producer，这样就不会有NotSerializableExceptions的问题了。
      lazy val producer = createProducer()
      def send(topic: String, key: K, value: V): Future[RecordMetadata] = 
        producer.send(new ProducerRecord[K, V](topic, key, value))
      def send(topic: String, value: V): Future[RecordMetadata] = 
        producer.send(new ProducerRecord[K, V](topic, value))
    }
    object KafkaPool{
      import scala.collection.JavaConversions._
      def apply[K, V](config: Map[String, Object]): KafkaPool[K, V] = {
          val createProducerFunc = () =&amp;gt; {
            //kafka权限认证
            System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;kafka_client_jaas.conf&quot;)
            val producer = new KafkaProducer[K, V](config)
            sys.addShutdownHook {
	      //当发送ececutor中的jvm shutdown时，kafka能够将缓冲区的消息发送出去。
              producer.close()
            }
            producer
          }
          new KafkaPool(createProducerFunc)
      }
      def apply[K, V](config: java.util.Properties): KafkaPool[K, V] = apply(config.toMap)
    }



  补充说明：
传值传参和传名的区别（()=&amp;gt;和:=&amp;gt;的区别）
  
    传值
        def test1(code: ()=&amp;gt;Unit){
    println(&quot;start&quot;)
    code() //要想调用传入的代码块，必须写成code()，否则不会调用。  
    println(&quot;end&quot;)
  }
  test1 {//此代码块，传入后立即执行。  
    println(&quot;when evaluated&quot;) //传入就打印
    ()=&amp;gt;{println(&quot;bb&quot;)} // 执行code()才打印
  }

      
      结果: 
when evaluated 
start
bb
end
    
    传名
        def test2(code : =&amp;gt; Unit){  
    println(&quot;start&quot;)  
    code // 这行才会调用传入的代码块，写成code()亦可  
    println(&quot;end&quot;)  
  }  
  test2{// 此处的代码块不会马上被调用  
    println(&quot;when evaluated&quot;)  //执行code的时候才调用
    println(&quot;bb&quot;) //执行code的时候才调用  
  }

      
      结果:
start
when evaluated
bb
end
    
  


第二步：利用广播变量下发KafkaProducer
利用广播变量，给每一个executor自己的KafkaProducer，将KafkaProducer广播到每一个executor中。  
注意：这里暂留一个问题，此种方式只可以Spark Streaming程序不用checkpoint的时候使用，否则会出现 “java.lang.ClassCastException:B cannot be cast to KafkaPool” 的问题，即使用这种方式直接做广播变量不可以，具体解决方式见下篇文章(SparkStreaming程序中checkpoint与广播变量兼容处理)。现在先说明这种不用checkpoint的方式。
在spark主程序中加入如下代码：
    //利用广播变量的形式，将后端写入KafkaProducer广播到每一个executor 注意：这里写广播变量的话，与checkpoint一起用会有问题
    val kafkaProducer: Broadcast[KafkaPool[String, String]] = {
      val kafkaProducerConfig = {
        val props = new Properties()
        props.put(&quot;metadata.broker.list&quot;,proKafkaBrokerAddr)
        props.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;)
        props.put(&quot;sasl.mechanism&quot;,&quot;PLAIN&quot;)
        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)
        props.put(&quot;bootstrap.servers&quot;,proKafkaBrokerAddr)
        props
      }
      log.warn(&quot;kafka producer init done!&quot;)
      ssc.sparkContext.broadcast(KafkaPool[String, String](kafkaProducerConfig))
    }


第三步：使用广播变量
spark 主程序中，在每个executor中使用广播变量。
    writeDStrem.foreachRDD(rdd =&amp;gt; {
      if (rdd.isEmpty) {
        logInfo(&quot; No Data in this batchInterval --------&quot;)
      } else {
        val start_time = System.currentTimeMillis()
        rdd.foreach(record=&amp;gt;{
           kafkaProducer.value.send(proKafkaTopicName,record)
        })
        competeTime(start_time, &quot;Processed data write to KAFKA&quot;)
      }
    })


至此，更改完毕。
结果对比
使用Kafka连接池更改程序之前以及之后的处理速度对比如下图所示，写入90W条数据由原来的17953ms变为了1966ms，效率大大提高。


内容即以上，会在下篇文章(SparkStreaming程序中checkpoint与广播变量兼容处理)解决spark streaming中checkpoint和广播变量使用冲突的问题，敬请期待。
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
©转载请注明
</content>
      <categories>
        
          <category> BigData </category>
        
      </categories>
      <tags>
        
          <tag> Spark </tag>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
