
<!doctype html>














<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Kerberos," />





  <link rel="alternate" href="/atom.xml" title="iLeaf" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="本文属于Kerberos具体实践整理的第二部分，主要涉及kerberos与HDFS的整合操作。">
<meta name="keywords" content="Kerberos">
<meta property="og:type" content="article">
<meta property="og:title" content="Kerberos具体实践2-Kerberos与HDFS的整合操作">
<meta property="og:url" content="http://localhost:4000/bigdata/2018/06/28/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B52-Kerberos%E4%B8%8EHDFS%E7%9A%84%E6%95%B4%E5%90%88%E6%93%8D%E4%BD%9C/">
<meta property="og:site_name" content="iLeaf">
<meta property="og:description" content="本文属于Kerberos具体实践整理的第二部分，主要涉及kerberos与HDFS的整合操作。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-06-28T00:00:00+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kerberos具体实践2-Kerberos与HDFS的整合操作">
<meta name="twitter:description" content="本文属于Kerberos具体实践整理的第二部分，主要涉及kerberos与HDFS的整合操作。">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://localhost:4000/"/>





  <title>Kerberos具体实践2-Kerberos与HDFS的整合操作 | iLeaf</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d509d2d894059ebd8e784708e52dcdc7";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>











</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">iLeaf</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Leaf's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

<div id="posts" class="posts-expand">
  
  

  

  
  
  

  <article class="post post-type- " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://localhost:4000/bigdata/2018/06/28/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B52-Kerberos%E4%B8%8EHDFS%E7%9A%84%E6%95%B4%E5%90%88%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="叶子  ( ˘ ³˘)♥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/assets/images/IMG_8739.GIF">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="iLeaf">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
          
          
            Kerberos具体实践2-Kerberos与HDFS的整合操作
          
        </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-28T00:00:00+08:00">
                2018-06-28
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-06-28">
                2018-06-28
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/#/BigData" itemprop="url" rel="index">
                    <span itemprop="name">BigData</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/bigdata/2018/06/28/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B52-Kerberos%E4%B8%8EHDFS%E7%9A%84%E6%95%B4%E5%90%88%E6%93%8D%E4%BD%9C/" class="leancloud_visitors" data-flag-title="Kerberos具体实践2-Kerberos与HDFS的整合操作">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          
            
                <div class="post-description">
                    本文属于Kerberos具体实践整理的第二部分，主要涉及kerberos与HDFS的整合操作。
                </div>
            
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
  <p>本文主要关于Kerberos的简单应用。因为工作测试需要，自己装了一套集群进行了Kerberos的部署，并且与HDFS、ZK进行整合，然后将操作过程进行了整理，以便后续再查看。本文主要涉及到其与HDFS的整合操作，此文为2018-05-05文章Kerberos具体实践1的后续，上文说明了Kerberos集群的安装以及基本命令的使用；由于篇幅有限，下文在2018-06-29文章Kerberos具体实践3中继续说明Kerberos与ZK整合操作。</p>
</blockquote>

<h1 id="hdfs整合kerberos部署">HDFS整合Kerberos（部署）</h1>
<h2 id="创建认证规则">创建认证规则</h2>
<p>在 Kerberos 安全机制里，一个principal就是realm里的一个对象，一个principal总是和一个密钥（secret key）成对出现的。<br />
这个 principal 的对应物可以是service，可以是host，也可以是user，对于Kerberos来说，都没有区别。<br />
Kdc(Key distribute center) 知道所有principal的secret key，但每个principal对应的对象只知道自己的那个secret key。这也是“共享密钥“的由来。<br />
对于hadoop，principals的格式为username/fully.qualified.domain.name@YOUR-REALM.COM。<br />
本次测试中，直接解压安装2.6.0-cdh5.11.0，并使用hadoop用户来进行NameNode和DataNode的启动，因此为集群中每个服务器节点添加principals：hadoop、HTTP、host（后续使用）。</p>
<blockquote>
  <p>补充：若通过yum源安装的cdh集群中，NameNode和DataNode是通过hdfs启动的，故为集群中每个服务器节点添加两个principals：hdfs、HTTP。-官网说法：The properties for each daemon (NameNode, Secondary NameNode, and DataNode) must specify both the HDFS and HTTP principals, as well as the path to the HDFS keytab file.</p>
</blockquote>

<p>在 KCD server 上（这里是 node1）创建 hadoop principal：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kadmin.local -q "addprinc -randkey hadoop/node1@HADOOP.COM"   
kadmin.local -q "addprinc -randkey hadoop/node2@HADOOP.COM"  
kadmin.local -q "addprinc -randkey hadoop/node3@HADOOP.COM"
</code></pre></div></div>

<p>创建 HTTP principal：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kadmin.local -q "addprinc -randkey HTTP/node1@HADOOP.COM" 
kadmin.local -q "addprinc -randkey HTTP/node2@HADOOP.COM"  
kadmin.local -q "addprinc -randkey HTTP/node3@HADOOP.COM"
</code></pre></div></div>

<blockquote>
  <p>说明：randkey 标志没有为新principal 设置密码，而是指示kadmin生成一个随机密钥。之所以在这里使用这个标志，是因为此principal不需要用户交互。它是计算机的一个服务器帐户。</p>
</blockquote>

<p>创建完成后，查看：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kadmin.local -q "listprincs"
</code></pre></div></div>

<p>操作结果小例子：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@node1 /]# kadmin.local -q "addprinc -randkey hadoop/node1@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for hadoop/node1@HADOOP.COM; defaulting to no policy
Principal "hadoop/node1@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "addprinc -randkey hadoop/node2@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for hadoop/node2@HADOOP.COM; defaulting to no policy
Principal "hadoop/node2@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "addprinc -randkey hadoop/node3@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for hadoop/node3@HADOOP.COM; defaulting to no policy
Principal "hadoop/node3@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "addprinc -randkey HTTP/node1@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for HTTP/node1@HADOOP.COM; defaulting to no policy
Principal "HTTP/node1@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "addprinc -randkey HTTP/node2@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for HTTP/node2@HADOOP.COM; defaulting to no policy
Principal "HTTP/node2@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "addprinc -randkey HTTP/node3@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for HTTP/node3@HADOOP.COM; defaulting to no policy
Principal "HTTP/node3@HADOOP.COM" created. 
[root@node1 hadoop_logs]# kadmin.local -q "addprinc -randkey host/node1@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for host/node1@HADOOP.COM; defaulting to no policy
Principal "host/node1@HADOOP.COM" created.
[root@node1 hadoop_logs]# kadmin.local -q "addprinc -randkey host/node2@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for host/node2@HADOOP.COM; defaulting to no policy
Principal "host/node2@HADOOP.COM" created.
[root@node1 hadoop_logs]# kadmin.local -q "addprinc -randkey host/node3@HADOOP.COM"
Authenticating as principal root/admin@HADOOP.COM with password.
WARNING: no policy specified for host/node3@HADOOP.COM; defaulting to no policy
Principal "host/node3@HADOOP.COM" created.
[root@node1 /]# kadmin.local -q "listprincs"
Authenticating as principal root/admin@HADOOP.COM with password.
HTTP/node1@HADOOP.COM
HTTP/node2@HADOOP.COM
HTTP/node3@HADOOP.COM
K/M@HADOOP.COM
hadoop/node1@HADOOP.COM
hadoop/node2@HADOOP.COM
hadoop/node3@HADOOP.COM
host/node1@HADOOP.COM
host/node2@HADOOP.COM
host/node3@HADOOP.COM
kadmin/admin@HADOOP.COM
kadmin/changepw@HADOOP.COM
kadmin/node1@HADOOP.COM
kiprop/node1@HADOOP.COM
krbtgt/HADOOP.COM@HADOOP.COM
root/admin@HADOOP.COM
</code></pre></div></div>
<h2 id="生成keytab">生成keytab</h2>
<p>keytab是包含principals和加密principal key的文件。keytab文件对于每个host是唯一的，因为key包含hostname。keytab文件用于不需要人工交互和保存纯文本密码，实现到kerberos上验证一个主机上的principal。因为服务器上可以访问keytab文件即可以以principal的身份通过 kerberos 的认证，所以，keytab文件应该被妥善保存，应该只有少数的用户可以访问。<br />
创建包含hadoop principal和HTTP principal的keytab。</p>
<ol>
  <li>方法一：<br />
在node1节点，即KDC server节点上执行下面命令，会在当前目录下创建keytab，名字为hdfs.keytab：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1、kadmin.local进入kerberos shell中
2、kadmin.local:  xst -norandkey -k hdfs.keytab hadoop/node1@HADOOP.COM hadoop/node2@HADOOP.COM hadoop/node3@HADOOP.COM HTTP/node1@HADOOP.COM HTTP/node2@HADOOP.COM HTTP/node3@HADOOP.COM
</code></pre></div>    </div>
    <p>使用klist显示hdfs.keytab文件列表,查看执行结果：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@node1 etc]# klist -ket  hdfs.keytab
Keytab name: FILE:hdfs.keytab
KVNO Timestamp           Principal
---- ------------------- ------------------------------------------------------
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 hadoop/node1@HADOOP.COM (des-cbc-md5)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 hadoop/node2@HADOOP.COM (des-cbc-md5)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 hadoop/node3@HADOOP.COM (des-cbc-md5)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 HTTP/node1@HADOOP.COM (des-cbc-md5)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 HTTP/node2@HADOOP.COM (des-cbc-md5)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (des3-cbc-sha1)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (arcfour-hmac)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (camellia256-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (camellia128-cts-cmac)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (des-hmac-sha1)
1 09/06/2017 17:53:54 HTTP/node3@HADOOP.COM (des-cbc-md5)
</code></pre></div>    </div>
  </li>
  <li>方法二（未使用）：<br />
在node1节点，即KDC server节点上执行下面命令，分别创建keytab，名字为hdfs.keytab：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd /var/kerberos/krb5kdc/  
$ kadmin.local -q "xst  -k hdfs-unmerged.keytab hadoop/node1@HADOOP.COM"  
$ kadmin.local -q "xst  -k hdfs-unmerged.keytab hadoop/node2@HADOOP.COM"  
$ kadmin.local -q "xst  -k hdfs-unmerged.keytab hadoop/node3@HADOOP.COM"  
$ kadmin.local -q "xst  -k HTTP.keytab  HTTP/node1@HADOOP.COM"  
$ kadmin.local -q "xst  -k HTTP.keytab  HTTP/node2@HADOOP.COM"  
$ kadmin.local -q "xst  -k HTTP.keytab  HTTP/node3@HADOOP.COM"  
</code></pre></div>    </div>
    <p>这样，就会在/var/kerberos/krb5kdc/（当前）目录下生成hdfs-unmerged.keytab和HTTP.keytab两个文件，接下来使用ktutil合并者两个文件为hdfs.keytab。</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd /var/kerberos/krb5kdc/   
$ ktutil  
ktutil: rkt hdfs-unmerged.keytab  
ktutil: rkt HTTP.keytab  
ktutil: wkt hdfs.keytab
</code></pre></div>    </div>
    <p>使用klist显示hdfs.keytab文件列表：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ klist -ket  hdfs.keytab
</code></pre></div>    </div>
  </li>
  <li>测试：<br />
验证是否正确合并了key，使用合并后的keytab，分别使用hadoop和HTTP principals来获取证书。
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kinit -k -t hdfs.keytab hadoop/node1@HADOOP.COM
$ kinit -k -t hdfs.keytab HTTP/node1@HADOOP.COM
</code></pre></div>    </div>
    <p>如果出现错误：kinit: Key table entry not found while getting initial credentials，则上面的（生成）合并有问题，重新执行前面的操作。</p>
  </li>
</ol>

<h2 id="部署kerberos-keytabhdfs">部署Kerberos Keytab（hdfs）</h2>
<p>拷贝生成的hdfs.keytab文件到其他节点的/etc/hadoop/conf目录【可以自己定义目录，后续需要在hdfs配置文件中使用此目录】</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd /var/kerberos/krb5kdc/  【刚刚生成keytab文件所在目录】
$ scp hdfs.keytab node1:/etc/hadoop/conf  
$ scp hdfs.keytab node2:/etc/hadoop/conf  
$ scp hdfs.keytab node3:/etc/hadoop/conf
</code></pre></div></div>
<p>并设置权限，分别在node1、node2、node3上执行更改文件属主和权限命令：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ssh node1 "chown hadoop:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab"
$ ssh node2 "chown hadoop:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab"
$ ssh node3 "chown hadoop:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab"
</code></pre></div></div>
<blockquote>
  <p>注意：由于keytab相当于有了永久凭证，不需要提供密码(如果修改kdc中的principal的密码，则该keytab就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab中指定的用户身份访问hadoop，所以keytab文件需要确保只对owner有读权限(0400)</p>
</blockquote>

<h2 id="hdfs配置修改">HDFS配置修改</h2>
<p>关于HDFS集群的部署配置就不在此文中说明，以下直接进行说明Kerberos整合HDFS之时所做的修改操作。</p>
<h3 id="修改core-sitexml配置文件">修改core-site.xml配置文件</h3>
<p>在集群中所有节点的core-site.xml文件中添加下面的配置:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.security.authentication<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>kerberos<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.security.authorization<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>
<h3 id="修改hdfs-sitexml配置文件">修改hdfs-site.xml配置文件</h3>
<p>在集群中所有节点的hdfs-site.xml文件中添加下面的配置：</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">&lt;!-- General HDFS security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.block.access.token.enable<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!-- NameNode security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- HDFS keytab的目录 --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.kerberos.internal.spnego.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!-- DataNode security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir.perm<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>700<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.address<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>0.0.0.0:61004<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.http.address<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>0.0.0.0:61006<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- HDFS keytab的目录 --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.kerberos.https.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!-- 开启SSL(jsvc时可不配置；可选，详见下属总结以及启动datanode部分) --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.http.policy<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTPS_ONLY<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="c">&lt;!-- SASL 模式--&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.data.transfer.protection<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>integrity<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!-- Web Authentication config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.webhdfs.enabled<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.web.authentication.kerberos.keytab<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- HDFS keytab的目录 --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.web.authentication.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!--- HDFS QJM HA：如果HDFS配置了QJM HA，则需要添加；另外，你还要在zookeeper上配置kerberos（详见后续文章说明） --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.kerberos.internal.spnego.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>
<h3 id="修改hadoop-envsh配置文件">修改hadoop-env.sh配置文件</h3>
<p>在hadoop-env.sh文件中，需要修改HADOOP_SECURE_DN_USER的值，其依据datanode启动方式不同，有不同的设置。<br />
注意的关键点如下：</p>
<ol>
  <li>jsvc模式启动datanode：HADOOP_SECURE_DN_USER有值</li>
  <li>SASL模式启动datanode：HADOOP_SECURE_DN_USER没值，要注释掉。</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}
#export HADOOP_SECURE_DN_USER=hadoop
</code></pre></div></div>
<p>具体配置方式，见下述启动datanode的部分，以不同的启动方式来详细说明。</p>

<p>配置中有几点要注意的：</p>
<ol>
  <li>如果不进行SSL配置，则hadoop需要使用jsvc来启动（如下条解释）。</li>
  <li>dfs.datanode.address、dfs.datanode.http.address表示data transceiver RPC server所绑定的hostname或IP地址，此部分配置与datanode的启动方式有关（本条内容目前是靠个人理解，由于目前还是菜鸟学习阶段，所以理解可能问题，请大家指出并告诉我，谢谢啦～我的邮箱leafming@foxmail.com），这部分的端口值有两种设置情况；
    <ul>
      <li>一定小于1024-jsvc:如果开启security，并且使用jsvc模式启动datanode（不配置SSL），dfs.datanode.address、dfs.datanode.http.address的端口一定要小于1024（privileged端口），否则的话启动datanode时候会报Cannot start secure cluster without privileged resources错误。CDH官网说明如下（详见<a href="https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cdh_sg_secure_hdfs_config.html#concept_nsy_21z_xn">Configure Secure HDFS</a>）：
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> The dfs.datanode.address and dfs.datanode.http.address port numbers for the DataNode must be below 1024,because this provides part of the security mechanism to make it impossible for a user to run a map task which impersonates a DataNode. 
 The port numbers for the NameNode and Secondary NameNode can be anything you want, but the default port numbers are good ones to use.
</code></pre></div>        </div>
      </li>
      <li>一定大于1024-SASL:如果使用在2.6之前的版本，安全模式的hadoop只能使用jsvc，即先以root用户来启动datanode，然后再切到普通用户。而在2.6.0之后的版本，SASL可以用来验证数据传输。在使用SASL的配置下，不再需要root用jsvc来启动安全模式的集群，并且也不在需要使用特权端口。如果启用SASL模式，需要在hdfs-site.xml中设置dfs.data.transfer.protection，并且dfs.datanode.address的端口要大于1024（non-privileged端口），并且设置dfs.http.policy为HTTPS_ONLY，而且保证HADOOP_SECURE_DN_USER环境变量值没有设置。一定要注意的是，dfs.datanode.address若设置成了特权端口，则SASL无法使用，这是向后兼容性原因所必需的。官网说明如下<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html">Hadoop in Secure Mode-Secure DataNode</a> ：
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> As of version 2.6.0, SASL can be used to authenticate the data transfer protocol. 
 In this configuration, it is no longer required for secured clusters to start the DataNode as root using jsvc and bind to privileged ports. To enable SASL on data transfer protocol, set dfs.data.transfer.
 protection in hdfs-site.xml, set a non-privileged port for dfs.datanode.address, set dfs.http.policy to HTTPS_ONLY and make sure the HADOOP_SECURE_DN_USER environment variable is not defined. 
 Note that it is not possible to use SASL on data transfer protocol if dfs.datanode.address is set to a privileged port. This is required for backwards-compatibility reasons.
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>principal中的instance部分可以使用_HOST标记，系统会自动替换它为全称域名。</li>
  <li>如果开启了security, hadoop会对hdfs block data(由dfs.data.dir指定)做permission check，方式用户的代码不是调用hdfs api而是直接本地读block data，这样就绕过了kerberos和文件权限验证，管理员可以通过设置dfs.datanode.data.dir.perm 来修改datanode 文件权限，这里我们设置为700。</li>
</ol>

<p>本次集群配置示例：</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>core-site.xml  
-----------------------------------------------------------------
<span class="nt">&lt;configuration&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hdfs://ns1<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>ha.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node1:2181,node2:2181,node3:2181<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>io.compression.codecs<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop-dir<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.security.authentication<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>kerberos<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.security.authorization<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
-----------------------------------------------------------------
hdfs-site.xml
-----------------------------------------------------------------
<span class="nt">&lt;configuration&gt;</span>
<span class="c">&lt;!-- General HDFS security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.block.access.token.enable<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.nameservices<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>ns1<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.ha.namenodes.ns1<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node1,node2<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.rpc-address.ns1.node1<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node1:8020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.rpc-address.ns1.node2<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node2:8020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.http-address.ns1.node1<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node1:8570<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.http-address.ns1.node2<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>node2:8570<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="c">&lt;!-- NameNode security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- path to the HDFS keytab --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.kerberos.internal.spnego.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="c">&lt;!-- DataNode security config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir.perm<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>700<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.address<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>0.0.0.0:61004<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.http.address<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>0.0.0.0:61006<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- path to the HDFS keytab --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.kerberos.https.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.http.policy<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTPS_ONLY<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.data.transfer.protection<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>integrity<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="c">&lt;!-- Web Authentication config --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.webhdfs.enabled<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.web.authentication.kerberos.keytab<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span> <span class="c">&lt;!-- path to the HTTP keytab --&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.web.authentication.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="c">&lt;!-- HDFS QJM HA--&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.keytab.file<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop_krb5/conf/hdfs.keytab<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.kerberos.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoop/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.kerberos.internal.spnego.principal<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>HTTP/_HOST@HADOOP.COM<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.shared.edits.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>qjournal://node1:8485;node2:8485;node3:8485/ns1<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.journalnode.edits.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/opt/hadoop-dir/journal<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.ha.fencing.methods<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>sshfence<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/home/hadoop/.ssh/id_rsa<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.permissions.enabled<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.acls.enabled<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.permissions.superusergroup<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>cgroup<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;final&gt;</span>true<span class="nt">&lt;/final&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>file:///opt/hadoop-dir/hadoop_data/nn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>file:///opt/hadoop-dir/hadoop_data/dn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>ha.zookeeper.session-timeout.ms<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>5000<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.ha.automatic-failover.enabled<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="nt">&lt;/name&gt;</span>  
  <span class="nt">&lt;value&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.socket.timeout<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>90000<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.balance.bandwidthPerSec<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>10485760<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
-----------------------------------------------------------------
</code></pre></div></div>
<h2 id="检查集群上的hdfs和本地文件的权限未尝试hdfs配置kerberos认证-文章中写需要此操作">检查集群上的HDFS和本地文件的权限（未尝试，<a href="https://yq.aliyun.com/articles/25636?spm=5176.100240.searchblog.8.wbY2wv">HDFS配置Kerberos认证</a> 文章中写需要此操作）</h2>
<p>请参考<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_sg_users_groups_verify.html?spm=5176.100239.blogcont25636.6.wkeCKj">Verify User Accounts and Groups in CDH 5 Due to Security</a>或者<a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SecureMode.html?spm=5176.100239.blogcont25636.7.wkeCKj">Hadoop in Secure Mode</a>。</p>

<h1 id="hdfs整合kerberos启动">HDFS整合Kerberos（启动）</h1>
<p>启动之前，请确认JCE jar已经替换，即若master_key_type和supported_enctypes使用aes256-cts（klist -e 命令查看采用了什么encryption），则需要替换JCE jar，请参考前面的说明。否则可能出现问题，可参考<a href="https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_sg_sec_troubleshooting.html#topic_17_1_5">The NameNode starts but clients cannot connect to it and error message contains enctype code 18</a>。
而本次操作中，没有使用aes256-cts，所以不用进行替换。</p>

<h2 id="kerberoskdc已经启动">KerberosKDC已经启动</h2>
<h2 id="启动namenode可不单独做这个操作直接配置好之后启动但可优先测试">启动NameNode（可不单独做这个操作，直接配置好之后启动，但可优先测试）</h2>
<h3 id="启动操作">启动操作</h3>
<p>上述更改配置文件的时候，已经停掉了全部hdfs服务，现在进行启动操作。<br />
在每个节点上获取root用户的ticket，这里root为之前创建的root/admin的密码。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ssh cdh1 "echo root@1234|kinit root/admin"
$ ssh cdh2 "echo root@1234|kinit root/admin"
$ ssh cdh3 "echo root@1234|kinit root/admin"
</code></pre></div></div>
<p>关闭selinux</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>setenforce 0
</code></pre></div></div>

<p>获取node1的启动namenode的ticket：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node1@HADOOP.COM
</code></pre></div></div>
<p>如果出现下面异常kinit: Password incorrect while getting initial credentials，则重新导出keytab再试试。</p>

<p>然后启动服务，观察日志： 
在node1启动namenode<br />
hadoop-daemon.sh start namenode</p>

<blockquote>
  <p>待全部配置好之后，hdfs ha启动可参看<a href="http://www.cnblogs.com/raphael5200/p/5154325.html">配置HDFS HA(高可用)</a></p>
</blockquote>

<h3 id="测试启动">测试启动</h3>
<p>在本次操作中，未直接进行测试，是在namenode和datanode全部启动之后才进行测试，但是，验证NameNode是否启动可参照下属操作：</p>
<ol>
  <li>打开 web 界面查看启动状态</li>
  <li>运行下面命令查看 hdfs（hadoop fs -ls /）：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ hadoop fs -ls /
 Found 4 items
 drwxrwxrwx   - yarn hadoop          0 2014-06-26 15:24 /logroot
 drwxrwxrwt   - hdfs hadoop          0 2014-11-04 10:44 /tmp
 drwxr-xr-x   - hdfs hadoop          0 2014-08-10 10:53 /user
 drwxr-xr-x   - hdfs hadoop          0 2013-05-20 22:52 /var
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="问题总结">问题总结</h3>
<p>采用此种方式过程中，可能遇到以下问题运行问题：</p>
<ol>
  <li>一个用户如果在你的凭据缓存中没有有效的kerberos ticket，执行上面命令将会失败，将会出现下面的错误：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     11/01/04 12:08:12 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
     GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     Bad connection to FS. command aborted. exception: Call to nn-host/10.0.0.2:8020 failed on local exception: java.io.IOException:
     javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
</code></pre></div>    </div>
    <p>解决方法：可通过klist来查看缓存状态，或重新kinit更新缓存。</p>
    <blockquote>
      <p>此问题官网说明参考<a href="https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_sg_sec_troubleshooting.html#topic_17_1_0">Running any Hadoop command fails after enabling security.</a></p>
    </blockquote>
  </li>
  <li>但是，如果使用的是MIT kerberos 1.8.1或更高版本、并且Oracle JDK 6 Update 26或更低版本，即使成功的使用kinit获取了ticket，java仍然无法读取kerberos票据缓存，可能出现如下错误：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     11/01/04 12:08:12 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
     GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     Bad connection to FS. command aborted. exception: Call to nn-host/10.0.0.2:8020 failed on local exception: java.io.IOException:
     javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
</code></pre></div>    </div>
    <p>问题原因：这个问题是因为MIT kerberos 1.8.1或更高版本在写凭证缓存的时候做了一个更改（change参考<a href="http://krbdev.mit.edu/rt/Ticket/Display.html?id=6206">MIT Kerberos Change</a>），如果使用是Oracle JDK 6 Update 26或更低版本，会遇到一个bug（bug参考<a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6979329">Report of bug in Oracle JDK 6 Update 26 and lower</a>），这个bug导致了在MIT kerberos 1.8.1或更高版中，java无法去读取Kerberos的票据缓存。而需要额外说明的是，Kerberos 1.8.1在Ubuntu Lucid或更高版本、 Debian Squeeze或更高版本中是默认Kerberos，会出现这个问题；但是在RHEL或CentOS中,默认的Kerberos是较老的版本，所以目前不会出现此问题。<br />
解决方法：在使用kinit获取ticket之后使用<strong><em>kinit -R</em></strong>来renew ticket。这样，将重写票据缓存中的ticket为java可读的格式，如下：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     $ klist
     klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_1000)
     $ hadoop fs -ls
     11/01/04 13:15:51 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
     GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     Bad connection to FS. command aborted. exception: Call to nn-host/10.0.0.2:8020 failed on local exception: java.io.IOException:
     javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     $ kinit
     Password for atm@YOUR-REALM.COM: 
     $ klist
     Ticket cache: FILE:/tmp/krb5cc_1000
     Default principal: atm@YOUR-REALM.COM
        
     Valid starting     Expires            Service principal
     01/04/11 13:19:31  01/04/11 23:19:31  krbtgt/YOUR-REALM.COM@YOUR-REALM.COM
        
     renew until 01/05/11 13:19:30
     $ hadoop fs -ls
     11/01/04 13:15:59 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
     GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     Bad connection to FS. command aborted. exception: Call to nn-host/10.0.0.2:8020 failed on local exception: java.io.IOException:
     javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
     $ kinit -R
     $ hadoop fs -ls
     Found 6 items
     drwx------   - atm atm          0 2011-01-02 16:16 /user/atm/.staging
</code></pre></div>    </div>
  </li>
  <li>在上述问题2中，也有kinit -R 问题：<br />
     但是，需要注意的是使用2这种方法，要求的是ticket是可renew的。而能否获取renewable tickets依赖于KDC的设置和每一个principal的设置（包括realm中有问题的principal和TGT service端的principal）。<br />
     如果，一个ticker不可renew，那么它的”valid starting”和”renew until”的值是相同的时间。这时，使用kinit -R就会遇到下面的问题：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     kinit: Ticket expired while renewing credentials
</code></pre></div>    </div>
    <blockquote>
      <p>此问题官网说明参考<a href="https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_sg_sec_troubleshooting.html#topic_17_1_1">Java is unable to read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher.</a></p>
    </blockquote>

    <p>所以为了获取可renew的ticket，可尝试使用以下方法-未彻底测试，若有问题，还需要大家自行解决下，并希望大家告诉我一下啦～（以下解决方法部分参考其他文章，<a href="http://blog.csdn.net/wulantian/article/details/42173095">CDH的Kerberos认证配置</a>，时间过去挺久了，有些遗忘，待有时间再重新测试下）：<br />
a.在kdc.conf中添加默认flag</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     default_principal_flags = +forwardable,+renewable
</code></pre></div>    </div>
    <p>但是实际没有起作用，因为查看资料，默认的principal_flags就包含了renewable，所以问题不是出在这里。另外需要说明一点，default_principal_flags只对这个flags生效以后创建的principal生效，之前创建的不生效，需要使用modprinc来使之前的principal生效。<br />
b.在kdc.conf中添加，并加大该参数：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     max_renewable_life = 10d
</code></pre></div>    </div>
    <p>修改之后重启kdc，重新kinit，再重新执行kinit -R则可以正常renew了。<br />
为了进行验证，再次将参数修改为“max_renewable_life = 0s”，再重新kinit后执行kinit -R则再次不能renew，则说明是否可以获取renew的ticket中，默认是可以获取renew的ticket的，但是，可以renw的最长时间是0s，所以造成无法renew，解决的办法是在kdc.conf中增大该参数。</p>
  </li>
</ol>

<blockquote>
  <p>另外补充：<br />
1」关于krb5.conf中的renew_lifetime = 7d参数，该参数设置该服务器上的使用kinit -R时renew的时间。<br />
2」可以通过modprinc来修改max_renewable_life的值，使用modprinc修改的值比kdc.conf中的配置有更高的优先级，例如，使用modprinc设置了为7天，kdc.conf中设置了为10天，使用getprinc可以看出，实际生效的是7天。需要注意的是，既要修改krbtgt/for_hadoop@for_hadoop，也要修改类似于hdfs/hadoop.local@for_hadoop这样的prinicials。使用modprinc来修改max_renewable_life，即kadmin.local进入kerberos的shell之后，执行：</p>
  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modprinc -maxrenewlife 7days krbtgt/for_hadoop@for_hadoop  
getprinc krbtgt/for_hadoop@for_hadoop
</code></pre></div>  </div>
</blockquote>

<p>到这里，kinit -R的问题解决，可以成功的执行hadoop fs -ls了。<br />
其他问题参考官网<a href="https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_sg_sec_troubleshooting.html#topic_17_1_0">Troubleshooting Authentication Issues</a>。</p>

<h2 id="启动datanode">启动datanode</h2>
<p>根据hdfs配置的区别，有两种启动datanode的方式，一种是使用jscv，一种是开启SASL。</p>
<h3 id="方式一jsvc启动datanode-目前此种方式测试有些问题能成功启动并正常使用但是启动后通过jps查看发现datanode处有进程号但无进程名字datanode原因未知待解决">方式一：jsvc启动datanode-目前此种方式测试有些问题，能成功启动并正常使用，但是启动后通过jps查看，发现datanode处有进程号，但无进程名字“datanode”，原因未知，待解决。</h3>
<p>如果没有配置TLS/SSL for HDFS（hdfs-site.xml里的dfs.http.policy），则datanode需要通过JSVC启动（只能root用户）。</p>

<p>关于Datanode+jsvc：<br />
在2.6版本前，启用hadoop安全模式，因为DataNode的数据传输协议没有使用HadoopRPC框架，DataNodes必须使用被dfs.datanode.address和dfs.datanode.http.address指定的特权端口来认证他们自己，该认证使基于假设攻击者无法获取在DataNode主机上的root特权。启动datanode的时候，必须用root用户，而当你使用root执行hdfs datanode命令时，服务器进程首先绑定特权端口，随后销毁特权并使用被HADOOP_SECURE_DN_USER指定的用户账号运行。这个启动进程使用被安装在JSVC_HOME的jsvc program。因此必须在启动项中hadoop-env.sh指定HADOOP_SECURE_DN_USER和JSVC_HOME做为环境变量。</p>

<p>jsvc模式启动datanode关键点先说明：</p>
<ul>
  <li>hdfs-site.xml中没有开启TLS/SSL HDFS配置；</li>
  <li>hdfs-site.xml中dfs.datanode.address、dfs.datanode.http.address的端口一定小于1024（特权端口）；</li>
  <li>
    <p>hadoop-env.sh设置HADOOP_SECURE_DN_USER为实际启动用户；</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  export HADOOP_SECURE_DN_USER=hadoop
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装jsvc，在hdfs-site.xml下加入安装目录JSVC_HOME，jsvc相关jar包替换；</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  export JSVC_HOME=/opt/hadoop/sbin/Linux/bigtop-jsvc-1.0.10-cdh5.11.0
</code></pre></div>    </div>
  </li>
  <li>启动服务前，先获取ticket再运行相关命令，要使用root用户启动datanode。</li>
</ul>

<h4 id="ijsvc环境部署hdfs集群的所有节点都要进行此操作">i.jsvc环境部署（hdfs集群的所有节点都要进行此操作）</h4>
<p>使用jsvc模式，hdfs-site.xml中去掉ssl配置，并且相关端口号使用特权端口，之后需要部署jsvc环境。
因为系统里未安装jsvc，则首先进行安装操作，安装参考<a href="http://blog.csdn.net/shuchangwen/article/details/45242549">jscv安装</a>。</p>
<ol>
  <li>下载安装包
因为我的hdoop版本是2.6.0-cdh5.11.0，因此我在<a href="http://archive.cloudera.com/cdh5/cdh/5/">CDH组件下载</a>处下载了bigtop-jsvc-1.0.10-cdh5.11.0.tar.gz。
    <blockquote>
      <p>补充：有些文章说明，apache hadoop可去<a href="http://archive.apache.org/dist/commons/daemon/binaries/">apache网站</a>下载源代码和bin包。并且，关于JSVC，默认指向hadoop安装目录的libexec下，但libexec下并没有jsvc文件（hadoop是直接下载的tar.gz包，不是rpm安装），如下载commons-daemon-1.0.15-src.tar.gz及commons-daemon-1.0.15-bin.tar.gz，先解压src包后进入src/native/unix目录依次(可能需要先yum install gcc make sdk autoconf，再执行)执行./configure命令，make命令，这样会在当前目录下生成一个叫jsvc的文件，之后把它拷贝到hadoop目录下的libexec下，或更改环境变量如3。</p>
    </blockquote>
  </li>
  <li>安装操作<br />
把安装包放到了HADOOP_HOME家目录”/sbin/Linux/”目录下，直接解压即可。<br />
解压成bigtop-jsvc-1.0.10-cdh5.11.0后，进入到$HADOOP_HOME/sbin/Linux/bigtop-jsvc-1.0.10-cdh5.11.0目录下，可以看到jsvc，使用“./jsvc -help”命令测试jsvc能否使用（注意修改整个文件的权限drwxr-xr-x. 5 hadoop root   4096 Sep  8 13:54 bigtop-jsvc-1.0.10-cdh5.11.0）。</li>
  <li>配置环境变量<br />
在hadoop-env.sh文件中加入以下内容：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export HADOOP_SECURE_DN_USER=hadoop
export JSVC_HOME=/opt/hadoop/sbin/Linux/bigtop-jsvc-1.0.10-cdh5.11.0
</code></pre></div>    </div>
    <blockquote>
      <p>说明：设置了HADOOP_SECURE_DN_USER的环境变量后，start-dfs.sh的启动脚本将会自动跳过DATANODE的启动。</p>
    </blockquote>
  </li>
  <li>替换jar包<br />
之前是没有进行替换jar包的操作，之后启动过程中出现错误，之后查阅资料得知，JSVC运行还需要一个commons-daemon-xxx.jar包，因此发现需要进行jar包替换，通过查阅资料得知，需要替换$HADOOP_HOME/share/hadoop/hdfs/lib下的commons-daemon-xxx.jar。<br />
因为我下载的是bigtop-jsvc-1.0.10-cdh5.11.0.tar.gz，而在HADOOP_HOME家目录的/share/hadoop/hdfs/lib下的是commons-daemon-1.0.13.jar，因此从<a href="http://archive.apache.org/dist/commons/daemon/binaries/">commons／daemon</a>下下载了commons-daemon-1.0.10.jar，然后把得到的commons-daemon-1.0.10.jar文件拷贝到hadoop安装目录下share/hadoop/hdfs/lib下（有文章说明要同时删除自带版本的commons-daemon-xxx.jar包，但我没有删除也可以使用）并更改了权限和其它内容保持一致(如-rwxr-xr-x. 1 hadoop root    24242 Sep  8 14:13 commons-daemon-1.0.10.jar)。</li>
</ol>

<blockquote>
  <p>接1补充：如果下载的是apache commons，即可解压bin包，然后把得到的commons-daemon-**.jar文件拷贝到hadoop安装目录下share/hadoop/hdfs/lib下，同时删除自带版本的commons-daemon-xxx.jar包。</p>
</blockquote>

<p>将上述操作在集群中的所有节点重复一遍（node1、node2、node3）。<br />
至此，jsvc的安装结束，注意hdfs的所有节点都要进行以上安装操作。</p>
<h4 id="ii启动集群datanode">ii.启动（集群）datanode</h4>
<h5 id="启动操作-1">启动操作</h5>
<p>设置了Security后，NameNode、QJM、ZKFC可以通过start-dfs.sh启动，DataNode需要使用（jsvc）root权限启动。<br />
并且因为设置了HADOOP_SECURE_DN_USER的环境变量后，执行start-dfs.sh的启动脚本将会自动跳过DATANODE的启动。<br />
所以使用jsvc模式，启动集群的时候需要两个步骤：</p>
<ol>
  <li>启动NameNode、QJM、ZKFC（详细参考上述启动namenode）（若不是第一次启动，第一次配置好了hadoop的启动需要参考<a href="http://www.cnblogs.com/raphael5200/p/5154325.html">配置HDFS HA(高可用)</a> ）
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#每台机器上（kinit启动的用户）
kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node1@HADOOP.COM
#node1上
start-dfs.sh
</code></pre></div>    </div>
    <p>使用start-dfs.sh之后查看QJM的日志和ZKFC的日志（QJM的报错不会有明显的提示）检查有无exception；如果启动不成功则仍需进行排查（keytab、zk的kerberos等）。</p>
  </li>
  <li>启动Datanode <br />
因为Datanode需要通过jsvc启动进程，当你使用root执行启动脚本（如hdfs datanode）命令时，服务器进程首先绑定特权端口，随后销毁特权并使用被HADOOP_SECURE_DN_USER指定的用户账号运行。<br />
所以先需要在集群中的每台机器上都获取原启动用户（hadoop）ticket（现在登陆的是root用户,时间有点长了，有些遗忘好像也需要kinitroot的），然后再启动服务。<br />
kinit并启动服务：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> # ssh node1 "kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node1@HADOOP.COM; ./opt/hadoop/sbin/hadoop-daemon.sh start datanode"
 # ssh node2 "kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node2@HADOOP.COM; ./opt/hadoop/sbin/hadoop-daemon.sh start datanode"
 # ssh node3 "kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node3@HADOOP.COM; ./opt/hadoop/sbin/hadoop-daemon.sh start datanode"
</code></pre></div>    </div>
    <p>补充，或在kinit后每台机器上使用命令启动：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> HADOOP_DATANODE_USER=hadoop sudo -E /opt/hadoop/bin/hdfs datanode
</code></pre></div>    </div>
    <p>或：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [root@node1 ~]# kinit -k -t /opt/hadoop_krb5/conf/hdfs.keytab hadoop/node1@HADOOP.COM
 [root@node1 ~]# start-secure-dns.sh
 node3: starting datanode, logging to /app/dcos/hadoop_logs//hadoop-hadoop-datanode-node3.out
 node1: starting datanode, logging to /app/dcos/hadoop_logs//hadoop-hadoop-datanode-node1.out
 node2: starting datanode, logging to /app/dcos/hadoop_logs//hadoop-hadoop-datanode-node2.out
    
</code></pre></div>    </div>
  </li>
  <li>验证datanode启动情况：观看node1上NameNode日志，出现下面日志表示DataNode启动成功：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2017-09-11 10:06:27,667 INFO org.apache.hadoop.security.UserGroupInformation: Login successful for user hadoop/node1@HADOOP.COM using keytab file /opt/hadoop_krb5/conf/hdfs.keytab
</code></pre></div>    </div>
  </li>
</ol>

<h5 id="遇到问题">遇到问题</h5>
<p>通过此方法启动datanode之后，通过jps查看，发现进程名字无法显示，但是hadoop集群运行正常。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@node1 sbin]# jps
6912 QuorumPeerMain
8785 NameNode
9251 DFSZKFailoverController
27156 Jps
10456
19929 Main
9033 JournalNode
</code></pre></div></div>
<p>排查问题补充相关：<br />
jps(Java Virtual Machine Process Status Tool)是提供的一个显示当前所有java进程pid的命令，而jvm运行时会生成一个目录hsperfdata_$USER(其中user是启动java进程的用户)，默认的是生成在 java.io.tmpdir目录下，在linux中默认是/tmp，目录下会有些pid文件,存放jvm进程信息可以利用strings查看里面的文件内容，一般就是jvm的进程信息而已。<br />
以上问题是只有进程号，没有进程名字，就思考是不是pid文件问题，于是去/tmp/hsperfdata_hadoop/和/tmp/hsperfdata_root/下看看是否有相关进程信息，发现datanode虽然是用jsvc方式用root用户启动的，而“10456”进程在/tmp/hsperfdata_hadoop/下。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@node1 tmp]# cd hsperfdata_
hsperfdata_apple1/  hsperfdata_apple2/  hsperfdata_hadoop/  hsperfdata_root/    hsperfdata_sitech1/ hsperfdata_sitech2/
[root@node1 tmp]# cd hsperfdata_hadoop/
[root@node1 hsperfdata_hadoop]# ls
10456  6912  8785  9033  9251
</code></pre></div></div>
<p>之后就去尝试使用SASL方式启动datanode了，这个问题就没有继续解决。怀疑是不是上面部署步骤出现问题了，有时间回来再看看是什么问题，再进行修改@TODO，希望大家有消息告诉我下，非常感谢！邮箱：leafming@foxmail.com。</p>
<blockquote>
  <p>此文“<a href="http://blog.csdn.net/xiao_jun_0820/article/details/39375819">Hadoop的kerberos的实践部署</a>”和“<a href="http://blog.csdn.net/wulantian/article/details/42173095">CDH的Kerberos认证</a>”内有关于datanode使用jsvc启动内容，但未完全参照。</p>
</blockquote>

<blockquote>
  <p>补充其他问题（好像这个问题没啥关系，不是这么解决，就先放这放着吧）：有人说，有时候端口问题服务无法正常启动，如：“WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: node2/10.211.55.11:8020”，使用“netstat -an|grep 8020”命令查看端口状态，如果不正常，映射为127.0.0.1:8020，则可尝试修改/etc/hosts文件中，去掉“127.0.0.1 localhost”，添加自己ip和主机名的映射，重新启动。</p>
</blockquote>

<h5 id="小总结">小总结</h5>
<p>加强理解，再次重复总结：jsvc模式启动datanode关键点：</p>
<ul>
  <li>hdfs-site.xml中没有开启TLS/SSL HDFS配置；</li>
  <li>hdfs-site.xml中dfs.datanode.address、dfs.datanode.http.address的端口一定小于1024（特权端口）；</li>
  <li>
    <p>hadoop-env.sh设置HADOOP_SECURE_DN_USER为实际启动用户；</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  export HADOOP_SECURE_DN_USER=hadoop
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装jsvc，在hdfs-site.xml下加入安装目录JSVC_HOME，jsvc相关jar包替换；</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  export JSVC_HOME=/opt/hadoop/sbin/Linux/bigtop-jsvc-1.0.10-cdh5.11.0
</code></pre></div>    </div>
  </li>
  <li>启动服务前，先获取ticket再运行相关命令，要使用root用户启动datanode，其它正常用hadoop用户即可。</li>
</ul>

<h3 id="方式二hfds中开启sasl整体配置">方式二：Hfds中开启SASL整体配置</h3>

<blockquote>
  <p>过渡说明：关于迁移一个使用root验证的hdfs集群为SASL方式，官方网站（<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html">Hadoop in Secure Mode-Secure DataNode</a> ）上有如下说明：</p>

  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In order to migrate an existing cluster that used root authentication to start using SASL instead, first ensure that version 2.6.0 or later has been deployed to all cluster nodes as well as any external applications that need to connect to the cluster. Only versions 2.6.0 and later of the HDFS client can connect to a DataNode that uses SASL for authentication of data transfer protocol, so it is vital that all callers have the correct version before migrating. After version 2.6.0 or later has been deployed everywhere, update configuration of any external applications to enable SASL. If an HDFS client is enabled for SASL, then it can connect successfully to a DataNode running with either root authentication or SASL authentication. Changing configuration for all clients guarantees that subsequent configuration changes on DataNodes will not disrupt the applications. Finally, each individual DataNode can be migrated by changing its configuration and restarting. It is acceptable to have a mix of some DataNodes running with root authentication and some DataNodes running with SASL authentication temporarily during this migration period, because an HDFS client enabled for SASL can connect to both.
</code></pre></div>  </div>
</blockquote>

<p>方式一介绍了使用jsvc启动datanode（稍有问题），下面开始介绍使用SASL的方式。</p>
<blockquote>
  <p>有文章写了sasl in hdfs的配置方式相关操作，参考<a href="http://blog.csdn.net/dxl342/article/details/55510659">HDFS使用Kerberos</a>。</p>
</blockquote>

<p>关于SASL模式的配置关键主要有以下几点，文章上面配置hadoop部分也有部分说明：</p>
<ul>
  <li>hdfs-site.xml中开启TLS/SSL HDFS配置；</li>
  <li>hdfs-site.xml中dfs.datanode.address、dfs.datanode.http.address的端口一定大于1024（非特权端口）；</li>
  <li>hadoop-env.sh中的HADOOP_SECURE_DN_USER内容一定为空，并且和jsvc模式不同，不需要进行jsvc的环境部署；</li>
  <li>https配置；</li>
  <li>正常启动hdfs集群。</li>
</ul>

<h4 id="ihdfs配置1基础配置">i.hdfs配置（1基础配置）</h4>
<p>虽然上面配置地方已经提过了，不过这次还是再重复写一遍吧，保持操作完整性嘛，大家配置过的可以去检查一次。</p>
<ol>
  <li>hdfs-site.xml中开启TLS/SSL HDFS配置，即在hdfs-site.xml中定义dfs.http.policy，指定要在HDFS的守护进程启动HTTPS服务器。增加如下内容：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;!-- 开启SSL(jsvc时可不配置；可选，详见下属总结以及启动datanode部分) --&gt;
 &lt;property&gt;
  	&lt;name&gt;dfs.http.policy&lt;/name&gt;
  	&lt;value&gt;HTTPS_ONLY&lt;/value&gt;
 &lt;/property&gt;
 &lt;!-- SASL 模式--&gt;
 &lt;property&gt;
  	&lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;
 	 &lt;value&gt;integrity&lt;/value&gt;
 &lt;/property&gt;
</code></pre></div>    </div>
    <p>说明，这里可以使用三个参数：  <br />
HTTP_ONLY: Only HTTP server is started<br />
HTTPS_ONLY: Only HTTPS server is started<br />
HTTP_AND_HTTPS: Both HTTP and HTTPS server are started  <br />
值得一提的是，如果dfs.http.policy设置了https_only，则普通的WebHDFS不再可用。您将需要使用WebHDFS基于HTTPS，在这种结构中，它能保护你通过WebHDFS的数据。</p>
  </li>
  <li>检查hdfs-site.xml中dfs.datanode.address、dfs.datanode.http.address的端口一定大于1024（非特权端口）；hadoop-env.sh中的HADOOP_SECURE_DN_USER内容一定为空。</li>
</ol>

<h4 id="iihttps配置">ii.https配置</h4>
<p>经过以上操作后，尝试启动hdfs，会发现namenode报错后退出，如下：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2017-09-13 14:12:37,273 INFO org.apache.hadoop.http.HttpServer2: HttpServer.start() threw a non Bind IOException
java.io.FileNotFoundException: /home/dtdream/.keystore (No such file or directory)
</code></pre></div></div>
<p>发现没有keystore文件，这个问题跟hadoop、Kerberos没什么关系，纯粹是https的配置了。<br />
简单说明https相关原理：https要求集群中有一个CA，它会生成ca_key和ca_cert，想要加入这个集群的节点，需要拿到这2个文件，然后经过一连串的动作生成keystore，并在hadoop的ssl-server.xml和ssl-client.xml中指定这个keystore的路径和密码，这样各个节点间就可以使用https进行通信了。
因此若要继续使用此模式，后续需要进行hdfs中的https配置。详细hdfs的https配置，可参考<a href="https://zh.hortonworks.com/blog/deploying-https-hdfs/">DEPLOYING HTTPS IN HDFS</a>，下面直接帖命令简单说明下过程。</p>

<p>注：在每台（node1、node2、node3）机器上都创建了/opt/ca 这个目录，存放ca相关文件，以下命令均在集群中各个机器的此目录下执行。</p>
<ol>
  <li>生成每个机器的密钥和证书<br />
首先需要在集群里面的每个机器上创建keyhecertificate，可以使用keytool命令来创建，如：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ keytool -keystore {keystore} -alias localhost -validity {validity} -genkey
</code></pre></div>    </div>
    <p>即通过此命令定义两个关键参数：<br />
keystore：keystore文件负责存储certificate，里面包括certificate的私钥，因此需要好好保存；<br />
validity：定义certificate的有效时间。<br />
keytool命令会设置certificate的一些相关细节，如CN、OU、O、L、ST、C；其中CN要和节点全域名（FQDN）保持一致，通常情况下，在有部署DNS的情况下，客户端将cn与DNS域名进行比较，以确保它确实连接到所需服务器，而不是恶意服务器。<br />
此操作命令示例（注意密码）：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#node1  ／opt／ca目录下
keytool -keystore keystore -alias node1 -validity 9999 -genkey -keyalg RSA -keysize 2048 -dname "CN=node1, OU=security, O=hadoop, L=beijing, ST=beijing, C=CN"  
#node2  ／opt／ca目录下
keytool -keystore keystore -alias node2 -validity 9999 -genkey -keyalg RSA -keysize 2048 -dname "CN=node1, OU=security, O=hadoop, L=beijing, ST=beijing, C=CN"  
#node3  ／opt／ca目录下
keytool -keystore keystore -alias node3 -validity 9999 -genkey -keyalg RSA -keysize 2048 -dname "CN=node1, OU=security, O=hadoop, L=beijing, ST=beijing, C=CN"
</code></pre></div>    </div>
    <p>执行完上述命令，会在当前目录下生成keystore文件。</p>
  </li>
  <li>创建自己的CA  <br />
经过第一步之后，集群的每个机器中有个自己的公私密钥对以及一个标识机器的证书。但是，证书是没有注册的，这意味着攻击者可以创建这样的证书冒充任何机器。<br />
因此，为了防止伪造证书，要为注册签名集群中的每台机器的证书。证书颁发机构（CA）负责签署证书。CA的工作就像一个政府签发护照，政府印章（标志）每个护照，使护照变得难以伪造。其他政府核实这些邮票以确保护照是真实的。类似地，CA在证书上签名，加密保证了签名证书难以伪造。因此，只要CA是一个真正可信的权威，客户就保证他们连接到真正的机器。<br />
下面，使用openssl命令去生成一个新的CA证书，生成的CA仅仅是一个公私密钥对和证书，可以用它签署其他证书。
我们这里将node1作为CA，在node1上执行如下命令（注意密码）：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#node1  ／opt／ca目录下
openssl req -new -x509 -keyout test_ca_key -out test_ca_cert -days 9999 -subj '/C=CN/ST=beijing/L=beijing/O=hadoop/OU=security/CN=node1'
</code></pre></div>    </div>
    <p>说明：这里cn是CA服务器的地址。<br />
执行完以上命令，会在当前目录下生成test_ca_cert、test_ca_key文件。</p>
  </li>
  <li>添加CA到客户机器的信任库<br />
在生成了CA之后，需要添加CA到每个客户端的信任库中，使客户可以信任这个CA，使用命令：
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ keytool -keystore {truststore} -alias CARoot -import -file {ca-cert}
</code></pre></div>    </div>
    <p>此步骤操作命令示例： 
即，首先在node1上，将上面生成的test_ca_key和test_ca_cert拷贝到集群中的所有机器上。</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#node1  ／opt／ca目录下
scp test_ca_cert node2:/opt/ca/  
scp test_ca_cert node3:/opt/ca/  
scp test_ca_key node3:/opt/ca/  
scp test_ca_key node2:/opt/ca/
</code></pre></div>    </div>
    <p>在集群中每台机器上执行如下命令（注意密码）：</p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#node1  ／opt／ca目录下  
keytool -keystore truststore -alias CARoot -import -file test_ca_cert
#node2  ／opt／ca目录下  
keytool -keystore truststore -alias CARoot -import -file test_ca_cert
#node3  ／opt／ca目录下  
keytool -keystore truststore -alias CARoot -import -file test_ca_cert
</code></pre></div>    </div>
    <p>执行完以上命令，会在当前目录下生成truststore文件。<br />
在步骤1中的ketstore存储的是每一台机器自己的身份，而客户机的truststore存储的是客户端信任的所有证书。导入证书到一个truststore中意味着信任那个证书签名的其它所有证书。如以上所述，信任政府（CA）也意味着信任它签发的所有护照（证书）。这个属性称为信任链，它在大型Hadoop集群上部署HTTPS时特别有用。你可以在集群中用一个CA签名所有认证，然后每个机器就能使用一个相同信任了CA的truststore，通过这种方式，所有的机器可以验证所有其他机器。</p>
  </li>
  <li>签名证书<br />
下一步就是要使用步骤2生成的CA来签名步骤1中生成的所有证书。
    <ol>
      <li>首先，你需要从密钥库中导出的证书，使用如下命令：
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ keytool -keystore -alias localhost -certreq -file {cert-file}
</code></pre></div>        </div>
        <p>即在node1、node2、node3上执行（注意密码）：</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1  ／opt／ca目录下
 keytool -certreq -alias node1 -keystore keystore -file cert  
 #node2  ／opt／ca目录下
 keytool -certreq -alias node2 -keystore keystore -file cert
 #node3  ／opt／ca目录下
 keytool -certreq -alias node3 -keystore keystore -file cert
</code></pre></div>        </div>
        <p>执行完以上命令，会在当前目录下生成cert文件。</p>
      </li>
      <li>之后，使用CA进行签名，使用如下命令：
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ openssl x509 -req -CA {ca-cert} -CAkey {ca-key} -in {cert-file} -out {cert-signed} -days {validity} -CAcreateserial -passin pass:{ca-password}
</code></pre></div>        </div>
        <p>即在node1、node2、node3上执行（注意密码）：</p>
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1  ／opt／ca目录下
 openssl x509 -req -CA test_ca_cert -CAkey test_ca_key -in cert -out cert_signed -days 9999 -CAcreateserial -passin pass:hadoop@1234
 #这个pass是CA的密码，即Then sign it with the CA。
 #node2  ／opt／ca目录下
 openssl x509 -req -CA test_ca_cert -CAkey test_ca_key -in cert -out cert_signed -days 9999 -CAcreateserial -passin pass:hadoop@1234
 #node3  ／opt／ca目录下
 openssl x509 -req -CA test_ca_cert -CAkey test_ca_key -in cert -out cert_signed -days 9999 -CAcreateserial -passin pass:hadoop@1234
</code></pre></div>        </div>
        <p>执行完以上命令，会在当前目录下生成cert_signed文件。</p>
      </li>
      <li>最后，将CA正式和被签名的证书都导入keystore中，使用如下命令：
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ keytool -keystore -alias CARoot -import -file {ca-cert}
 $ keytool -keystore -alias localhost -import -file {cert-signed}
</code></pre></div>        </div>
        <p>即在node1、node2、node3上执行（注意密码）：</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1  ／opt／ca目录下
 keytool -keystore keystore -alias CARoot -import -file test_ca_cert
 keytool -keystore keystore -alias node1 -import -file cert_signed
 #node2  ／opt／ca目录下
 keytool -keystore keystore -alias CARoot -import -file test_ca_cert
 keytool -keystore keystore -alias node2 -import -file cert_signed
 #node3  ／opt／ca目录下
 keytool -keystore keystore -alias CARoot -import -file test_ca_cert
 keytool -keystore keystore -alias node3 -import -file cert_signed
</code></pre></div>        </div>
      </li>
      <li>补充：以上命令参数说明：<br />
 keystore:keystore的位置<br />
 ca-cert:CA证书<br />
 ca-key:CA私钥<br />
 ca-password:CA密码<br />
 cert-file:导出的服务器的未签名证书<br />
 cert-signed:服务器的签名的证书</li>
    </ol>
  </li>
</ol>

<h4 id="iiihdfs配置2整合https">iii.hdfs配置（2整合https）</h4>
<p>最后需要去配置HDFS中的HTTPs。<br />
其次，需要配置在$HADOOP_HOME/etc/hadoop目录下的ssl-server.xml和ssl-client.xml，告诉HDFS的密钥库和信任存储区。<br />
在所有节点上均进行此配置，现在node1上配置好，之后拷贝到其他节点（因为文件设置的都是在一样的路径）。<br />
说明：这里没来得及深究，有些配置可以省略。以后又时间再补充。@TODO</p>
<ol>
  <li>ssl-client.xml<br />
本次操作配置文件示例：
    <div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.truststore.location<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>/opt/ca/truststore<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Truststore to be used by clients like distcp. Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.truststore.password<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. Default value is "".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.truststore.type<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>jks<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. The keystore file format, default value is "jks".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.truststore.reload.interval<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>10000<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Truststore reload check interval, in milliseconds.
   Default value is 10000 (10 seconds).
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.keystore.location<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>/opt/ca/keystore<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Keystore to be used by clients like distcp. Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.keystore.password<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. Default value is "".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.keystore.keypassword<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. Default value is "".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.client.keystore.type<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>jks<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. The keystore file format, default value is "jks".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>    </div>
  </li>
  <li>ssl-server.xml<br />
本次操作配置文件示例：
    <div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.truststore.location<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>/opt/ca/truststore<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Truststore to be used by NN and DN. Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.truststore.password<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. Default value is "".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.truststore.type<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>jks<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. The keystore file format, default value is "jks".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.truststore.reload.interval<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>10000<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Truststore reload check interval, in milliseconds.
   Default value is 10000 (10 seconds).
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.keystore.location<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>/opt/ca/keystore<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Keystore to be used by NN and DN. Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.keystore.password<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.keystore.keypassword<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoop@1234<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Must be specified.
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.keystore.type<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>jks<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. The keystore file format, default value is "jks".
   <span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ssl.server.exclude.cipher.list<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,SSL_RSA_WITH_RC4_128_MD5<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Optional. The weak security cipher suites that you want excluded from SSL communication.<span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>    </div>
    <p>以上，关于Hfds中开启SASL整体配置说明完毕。</p>
  </li>
</ol>

<p>下面总结一下都做了什么：</p>
<ul>
  <li>hdfs-site.xml中开启TLS/SSL HDFS配置；</li>
  <li>hdfs-site.xml中dfs.datanode.address、dfs.datanode.http.address的端口一定大于1024（非特权端口）；</li>
  <li>hadoop-env.sh中的HADOOP_SECURE_DN_USER内容一定为空，并且和jsvc模式不同，不需要进行jsvc的环境部署；</li>
  <li>配置https：生成每个机器的密钥和证书；创建自己的CA；添加CA到客户机器的信任库；签名证书；</li>
  <li>hdfs中整合https：配置ssl-server.xml和ssl-client.xml。</li>
</ul>

<h4 id="iv测试启动集群">iv.测试启动（集群）</h4>
<p>以上，关于hdfs中开启SASL的整体配置完毕，下面即可直接进行集群的启动。<br />
补充：此处作为补充完整描述下全新集群的启动过程，通常情况下，直接启动即可。</p>
<ol>
  <li>全新HA集群启动过程：
    <ol>
      <li>kinit验证</li>
      <li>
        <p>分别启动zookeeper</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> zkServer.sh start
</code></pre></div>        </div>
      </li>
      <li>
        <p>启动三台JournalNode：node1、node2、node3</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> hadoop-daemon.sh start journalnode
</code></pre></div>        </div>
      </li>
      <li>在其中一个NameNode上格式化hadoop.tmp.dir并初始化(格式化完成之后，将会在<script type="math/tex">\$\{dfs.namenode.name.dir\}/current</script>目录下生成元数据，<script type="math/tex">\$\{dfs.namenode.name.dir\}</script>是在hdfs-site.xml中配置的，默认值是<script type="math/tex">file://\$\{hadoop.tmp.dir\}/dfs/name</script>，dfs.namenode.name.dir属性可配多个目录，如 /data1/dfs/name，/data2/dfs/name，/data3/dfs/name等。各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统 Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存；<script type="math/tex">\$\{hadoop.tmp.dir\}</script>是在core-si te.xml文件内配置的）
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1
 hdfs namenode -format
</code></pre></div>        </div>
      </li>
      <li>把格式化后的元数据拷备到另一台NameNode节点上
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1
 $ scp -r ${hadoop.tmp.dir} root@node2:${hadoop.tmp.dir}
</code></pre></div>        </div>
      </li>
      <li>
        <p>启动NameNode</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #node1
 hadoop-daemon.sh start namenode
 #node2
 hdfs namenode -bootstrapStandby（Y/N？）
 hadoop-daemon.sh start namenode
</code></pre></div>        </div>
      </li>
      <li>
        <p>初始化zkfc</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #Node1:
 hdfs zkfc -formatZK
</code></pre></div>        </div>
      </li>
      <li>
        <p>全面停止并全面启动</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> #Node1:
 stop-dfs.sh
 start-dfs.sh
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
  <li>因为启动过程中用到了hadoopHA，因此，可测试namenode的HA切换。
    <ol>
      <li>
        <p>用命令查看namenode的状态（两台namenode，在hdfs-si te.xml中配置的名字和主机名保持了一直，也是node1，所以这里测试写node1）：</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> hdfs haadmin -getServiceState node1
</code></pre></div>        </div>
      </li>
      <li>
        <p>通过使用上述命令，有时候出现两台namenode都是standby的情况，则可以使用如下命令进行强制切换：</p>

        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> hdfs haadmin -transitionToActive --forcemanual node1
</code></pre></div>        </div>
      </li>
      <li>
        <p>再次使用上述命令查看切换状态。</p>
      </li>
    </ol>
  </li>
</ol>

<hr />
<p>至此，本篇内容完成。以上内容基本上是完全关于HDFS中的Kerberos部署，HDFS结合Kerberos的整体部署完毕，若配置中使用了HA，则需要进行下文ZK配置之后才能完成完整部署。</p>


      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
如有问题，请发送邮件至leafming@foxmail.com联系我，谢谢～
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      叶子  ( ˘ ³˘)♥
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://localhost:4000/bigdata/2018/06/28/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B52-Kerberos%E4%B8%8EHDFS%E7%9A%84%E6%95%B4%E5%90%88%E6%93%8D%E4%BD%9C/" title="Kerberos具体实践2-Kerberos与HDFS的整合操作">Kerberos具体实践2-Kerberos与HDFS的整合操作</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            
            <a href="/tag/#/Kerberos" rel="tag"># Kerberos</a>
          
        </div>
      

      
      
      
      
      

      
      
        <div class="post-nav" id="post-nav-id">
          <div class="post-nav-next post-nav-item">
            
              <a href="/bigdata/2018/06/29/Kerberos%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B53-Kerberos%E4%B8%8EZK%E7%9A%84%E6%95%B4%E5%90%88%E6%93%8D%E4%BD%9C/" rel="next" title="Kerberos具体实践3-Kerberos与ZK的整合操作">
                <i class="fa fa-chevron-left"></i> Kerberos具体实践3-Kerberos与ZK的整合操作
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/cloud/2018/06/15/Docker+Mesos+Marathon%E7%9B%91%E6%8E%A7%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="prev" title="Docker+Mesos+Marathon监控方法使用总结">
                Docker+Mesos+Marathon监控方法使用总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      
      

      
    </footer>
  </article>

  <div class="post-spread">
    
  </div>
</div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNTIzNi8xMTc3Mg=="></div>
    
  </div>


        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        
        
        




      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/assets/images/IMG_8739.GIF"
               alt="叶子  ( ˘ ³˘)♥" />
          <p class="site-author-name" itemprop="name">叶子  ( ˘ ³˘)♥</p>
           
              <p class="site-description motion-element" itemprop="description">Yesterday you said tomorrow.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              
              
              <span class="links-of-author-item">
                <a href="mailto:leafming@foxmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  E-Mail
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/assets/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                
                
                <li class="links-of-blogroll-item">
                  <a href="https://www.boyandebate.club/" title="576’sBlog" target="_blank">576’sBlog</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            





            
              <div class="post-toc-content">
    <ol class=nav>
      <li class="nav-item nav-level-1"> <a class="nav-link" href="#hdfs整合kerberos部署"> <span class="nav-number">1</span> <span class="nav-text">HDFS整合Kerberos（部署）</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-2"> <a class="nav-link" href="#创建认证规则"> <span class="nav-number">1.1</span> <span class="nav-text">创建认证规则</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#生成keytab"> <span class="nav-number">1.2</span> <span class="nav-text">生成keytab</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#部署kerberos-keytabhdfs"> <span class="nav-number">1.3</span> <span class="nav-text">部署Kerberos Keytab（hdfs）</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#hdfs配置修改"> <span class="nav-number">1.4</span> <span class="nav-text">HDFS配置修改</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-3"> <a class="nav-link" href="#修改core-sitexml配置文件"> <span class="nav-number">1.4.1</span> <span class="nav-text">修改core-site.xml配置文件</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#修改hdfs-sitexml配置文件"> <span class="nav-number">1.4.2</span> <span class="nav-text">修改hdfs-site.xml配置文件</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#修改hadoop-envsh配置文件"> <span class="nav-number">1.4.3</span> <span class="nav-text">修改hadoop-env.sh配置文件</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#检查集群上的hdfs和本地文件的权限未尝试hdfs配置kerberos认证-文章中写需要此操作"> <span class="nav-number">1.5</span> <span class="nav-text">检查集群上的HDFS和本地文件的权限（未尝试，HDFS配置Kerberos认证 文章中写需要此操作）</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-1"> <a class="nav-link" href="#hdfs整合kerberos启动"> <span class="nav-number">2</span> <span class="nav-text">HDFS整合Kerberos（启动）</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-2"> <a class="nav-link" href="#kerberoskdc已经启动"> <span class="nav-number">2.1</span> <span class="nav-text">KerberosKDC已经启动</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#启动namenode可不单独做这个操作直接配置好之后启动但可优先测试"> <span class="nav-number">2.2</span> <span class="nav-text">启动NameNode（可不单独做这个操作，直接配置好之后启动，但可优先测试）</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-3"> <a class="nav-link" href="#启动操作"> <span class="nav-number">2.2.1</span> <span class="nav-text">启动操作</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#测试启动"> <span class="nav-number">2.2.2</span> <span class="nav-text">测试启动</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#问题总结"> <span class="nav-number">2.2.3</span> <span class="nav-text">问题总结</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#启动datanode"> <span class="nav-number">2.3</span> <span class="nav-text">启动datanode</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-3"> <a class="nav-link" href="#方式一jsvc启动datanode-目前此种方式测试有些问题能成功启动并正常使用但是启动后通过jps查看发现datanode处有进程号但无进程名字datanode原因未知待解决"> <span class="nav-number">2.3.1</span> <span class="nav-text">方式一：jsvc启动datanode-目前此种方式测试有些问题，能成功启动并正常使用，但是启动后通过jps查看，发现datanode处有进程号，但无进程名字“datanode”，原因未知，待解决。</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#ijsvc环境部署hdfs集群的所有节点都要进行此操作"> <span class="nav-number">2.3.1.1</span> <span class="nav-text">i.jsvc环境部署（hdfs集群的所有节点都要进行此操作）</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#ii启动集群datanode"> <span class="nav-number">2.3.1.2</span> <span class="nav-text">ii.启动（集群）datanode</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-5"> <a class="nav-link" href="#启动操作-1"> <span class="nav-number">2.3.1.2.1</span> <span class="nav-text">启动操作</span> </a> <ol class="nav-child"> </li></ol> </li> <li class="nav-item nav-level-5"> <a class="nav-link" href="#遇到问题"> <span class="nav-number">2.3.1.2.2</span> <span class="nav-text">遇到问题</span> </a> <ol class="nav-child"> </li></ol> </li> <li class="nav-item nav-level-5"> <a class="nav-link" href="#小总结"> <span class="nav-number">2.3.1.2.3</span> <span class="nav-text">小总结</span> </a> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#方式二hfds中开启sasl整体配置"> <span class="nav-number">2.3.2</span> <span class="nav-text">方式二：Hfds中开启SASL整体配置</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#ihdfs配置1基础配置"> <span class="nav-number">2.3.2.1</span> <span class="nav-text">i.hdfs配置（1基础配置）</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#iihttps配置"> <span class="nav-number">2.3.2.2</span> <span class="nav-text">ii.https配置</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#iiihdfs配置2整合https"> <span class="nav-number">2.3.2.3</span> <span class="nav-text">iii.hdfs配置（2整合https）</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#iv测试启动集群"> <span class="nav-number">2.3.2.4</span> <span class="nav-text">iv.测试启动（集群）</span> </a> <ol class="nav-child"> <ol class="nav-child">
    </ol>
  </div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">叶子  ( ˘ ³˘)♥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://jekyllrb.com">Jekyll</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  











  




  

    
      <script type="text/javascript">
        (function(d, s) {
          var j, e = d.getElementsByTagName(s)[0];
          if (typeof LivereTower === 'function') { return; }
          j = d.createElement(s);
          j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
          j.async = true;
          e.parentNode.insertBefore(j, e);
        })(document, 'script');
      </script>
    

  





  


  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("c82MFNFEU6p4iaz5Ik4L07e5-gzGzoHsz", "CSzcJkq7WGpn5bNMendMuKYf");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

